{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eff253f",
   "metadata": {},
   "source": [
    "\n",
    "# Proyecto: Aumento de Imágenes y CNN con **Sign Language MNIST** (CSV)\n",
    "\n",
    "**Objetivo**  \n",
    "- Cargar el dataset _Sign Language MNIST_ desde CSV.  \n",
    "- Aplicar **≥ 6 transformaciones** (rotación, traslación, escalado, inversión, ruido, recorte aleatorio…).  \n",
    "- Implementar una **CNN en PyTorch** (≥ 2 capas `Conv2d` y capas de `Pooling`) explorando **kernel size, stride, padding y pooling**.  \n",
    "- Entrenar durante **≥ 5 épocas** con **Adam**, mostrando **pérdida y precisión** en **train** y **valid** tras cada época.  \n",
    "- Desarrollar todo en este **Notebook** para subirlo a GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f9134d",
   "metadata": {},
   "source": [
    "\n",
    "## Requisitos previos\n",
    "\n",
    "1. Descarga desde Kaggle el dataset **Sign Language MNIST** en CSV (p. ej. `sign_mnist_train.csv` y `sign_mnist_test.csv`).  \n",
    "2. Sube el/los CSV(s) a la carpeta del entorno donde ejecutes este Notebook (Colab / local).  \n",
    "3. Si estás en Windows y `num_workers=2` da problemas, pon `num_workers=0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd9f22",
   "metadata": {},
   "source": [
    "link para descargar dataset -> https://www.kaggle.com/datasets/datamunge/sign-language-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d32bdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Imports principales\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Config reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f8a57",
   "metadata": {},
   "source": [
    "\n",
    "## Parte 1 · Carga desde CSV (imágenes 28×28 en escala de grises)\n",
    "\n",
    "Cada fila del CSV contiene una imagen 28×28 (784 columnas) + una columna `label` con la clase (A–Y excepto J y Z en `train`).  \n",
    "Transformaremos los píxeles a `float32` y normalizaremos a \\[0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545cf936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV de entrenamiento encontrado.\n",
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      3     107     118     127     134     139     143     146     150   \n",
      "1      6     155     157     156     156     156     157     156     158   \n",
      "2      2     187     188     188     187     187     186     187     188   \n",
      "3      2     211     211     212     212     211     210     211     210   \n",
      "4     13     164     167     170     172     176     179     180     184   \n",
      "\n",
      "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0     153  ...       207       207       207       207       206       206   \n",
      "1     158  ...        69       149       128        87        94       163   \n",
      "2     187  ...       202       201       200       199       198       199   \n",
      "3     210  ...       235       234       233       231       230       226   \n",
      "4     185  ...        92       105       105       108       133       163   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0       206       204       203       202  \n",
      "1       175       103       135       149  \n",
      "2       198       195       194       195  \n",
      "3       225       222       229       163  \n",
      "4       157       163       164       179  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((27455, 1, 28, 28), (27455,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Ruta a los CSV (ajusta si hace falta)\n",
    "TRAIN_CSV = 'dataset/sign_mnist_train.csv'  # cambiar si tu archivo tiene otro nombre\n",
    "TEST_CSV  = 'datset/sign_mnist_test.csv'   # opcional para evaluación extra\n",
    "\n",
    "assert os.path.exists(TRAIN_CSV), f\"No se encontró {TRAIN_CSV}. Sube el CSV a esta carpeta.\"\n",
    "print(\"CSV de entrenamiento encontrado.\")\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "print(df_train.head())\n",
    "\n",
    "labels = df_train['label'].values.astype(np.int64)\n",
    "X = df_train.drop(columns='label').to_numpy().astype(np.float32)\n",
    "# Reescalar a [0,1]\n",
    "X = X / 255.0\n",
    "# Reformar a (N, 1, 28, 28)\n",
    "X = X.reshape(-1, 1, 28, 28)\n",
    "X.shape, labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be838d",
   "metadata": {},
   "source": [
    "\n",
    "## Selección de una imagen y **≥ 6 transformaciones**\n",
    "\n",
    "Aplicamos transformaciones de `torchvision.transforms` y una transformación personalizada de **ruido gaussiano**.  \n",
    "Mostramos la imagen original junto a 6 versiones transformadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f7dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta de la imagen seleccionada: 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pic should not have > 4 channels. Got 28 channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 5 PIL-based\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tf \u001b[38;5;129;01min\u001b[39;00m tfs[:\u001b[32m5\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     transformed.append(\u001b[43mapply_pil_tf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# RandomCrop también es PIL-based\u001b[39;00m\n\u001b[32m     44\u001b[39m transformed.append(apply_pil_tf(img0, tfs[\u001b[32m5\u001b[39m]))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mapply_pil_tf\u001b[39m\u001b[34m(img_tensor, pil_tf)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_pil_tf\u001b[39m(img_tensor, pil_tf):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     pil = \u001b[43mto_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# (1,28,28) -> PIL (L)\u001b[39;00m\n\u001b[32m     35\u001b[39m     out = pil_tf(pil)             \u001b[38;5;66;03m# PIL -> PIL\u001b[39;00m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m to_tensor(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juanq\\Desktop\\repo\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:234\u001b[39m, in \u001b[36mToPILImage.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m \n\u001b[32m    233\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juanq\\Desktop\\repo\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:277\u001b[39m, in \u001b[36mto_pil_image\u001b[39m\u001b[34m(pic, mode)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.shape[-\u001b[32m1\u001b[39m] > \u001b[32m4\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpic should not have > 4 channels. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic.shape[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m channels.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    279\u001b[39m npimg = pic\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.issubdtype(npimg.dtype, np.floating) \u001b[38;5;129;01mand\u001b[39;00m mode != \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: pic should not have > 4 channels. Got 28 channels."
     ]
    }
   ],
   "source": [
    "\n",
    "# Selecciona una imagen (por ejemplo la primera)\n",
    "idx = 0\n",
    "img0 = X[idx]  # (1, 28, 28) en [0,1]\n",
    "label0 = labels[idx]\n",
    "print(\"Etiqueta de la imagen seleccionada:\", label0)\n",
    "\n",
    "# Definimos 6+ transformaciones (sobre PIL o Tensor). Usaremos PIL para compatibilidad amplia:\n",
    "to_pil = transforms.ToPILImage()\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=0.1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
    "        out = tensor + noise\n",
    "        return torch.clamp(out, 0.0, 1.0)\n",
    "\n",
    "# Conjunto de transformaciones sugeridas\n",
    "tfs = [\n",
    "    transforms.RandomRotation(degrees=25),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),          # traslación\n",
    "    transforms.RandomAffine(degrees=0, scale=(0.7, 1.3)),              # escalado\n",
    "    transforms.RandomHorizontalFlip(p=1.0),                             # espejo horizontal\n",
    "    transforms.RandomInvert(p=1.0),                                     # inversión de color\n",
    "    transforms.RandomCrop(size=24, padding=2),                          # recorte aleatorio\n",
    "]\n",
    "\n",
    "# Añadimos ruido gaussiano como transform tensor-based\n",
    "noise_tf = transforms.Compose([to_tensor, AddGaussianNoise(0.0, 0.2)])\n",
    "# Para el resto (PIL-based): ToPIL -> tf -> ToTensor\n",
    "def apply_pil_tf(img_tensor, pil_tf):\n",
    "    pil = to_pil(img_tensor)      # (1,28,28) -> PIL (L)\n",
    "    out = pil_tf(pil)             # PIL -> PIL\n",
    "    return to_tensor(out)         # PIL -> (1,H,W) [0,1]\n",
    "\n",
    "# Generar versiones transformadas\n",
    "transformed = []\n",
    "# 5 PIL-based\n",
    "for tf in tfs[:5]:\n",
    "    transformed.append(apply_pil_tf(img0, tf))\n",
    "# RandomCrop también es PIL-based\n",
    "transformed.append(apply_pil_tf(img0, tfs[5]))\n",
    "# Ruido (tensor-based)\n",
    "transformed.append(noise_tf(img0))\n",
    "\n",
    "len(transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e13612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualización: original + 6 transformaciones (7 imágenes)\n",
    "fig, axes = plt.subplots(1, 7, figsize=(18, 3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(img0.squeeze(), cmap='gray')\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "titles = [\"Rotación\", \"Traslación\", \"Escalado\", \"Flip H\", \"Invertir\", \"Crop\", \"Ruido\"]\n",
    "for i, (ax, timg, title) in enumerate(zip(axes[1:], transformed, titles)):\n",
    "    ax.imshow(timg.squeeze(), cmap='gray')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd75037",
   "metadata": {},
   "source": [
    "\n",
    "## Parte 2 · Dataset, Split y DataLoader\n",
    "\n",
    "- Convertimos `X, y` a tensores.  \n",
    "- Split **80/20** en _train/valid_.  \n",
    "- `DataLoader` con `batch_size=32` y `num_workers=2` (ajusta a `0` si Windows da problemas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tensores\n",
    "X_tensor = torch.from_numpy(X)            # (N,1,28,28), float32\n",
    "y_tensor = torch.from_numpy(labels)       # (N,), int64\n",
    "\n",
    "class SLMDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "full_ds = SLMDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split 80/20\n",
    "N = len(full_ds)\n",
    "n_train = int(0.8 * N)\n",
    "n_val = N - n_train\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2  # si en Windows falla, usa 0\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9f81c",
   "metadata": {},
   "source": [
    "\n",
    "## Exploración de tamaños de salida en **Conv2d** y **Pooling**\n",
    "\n",
    "Función auxiliar para calcular tamaños de salida al variar **kernel_size**, **stride**, **padding** y **pooling**.  \n",
    "Fórmula para una dimensión:  \n",
    "\\[ \\text{out} = \\left\\lfloor \\frac{\\text{in} + 2\\,\\text{padding} - \\text{kernel}}{\\text{stride}} \\right\\rfloor + 1 \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_out_size(in_size, kernel, stride=1, padding=0):\n",
    "    return (in_size + 2*padding - kernel)//stride + 1\n",
    "\n",
    "def pool_out_size(in_size, kernel, stride=None, padding=0):\n",
    "    if stride is None:\n",
    "        stride = kernel\n",
    "    return (in_size + 2*padding - kernel)//stride + 1\n",
    "\n",
    "# Ejemplos de combinaciones\n",
    "input_h = input_w = 28\n",
    "for k in [3,5]:\n",
    "    for p in [0,1]:\n",
    "        for s in [1,2]:\n",
    "            out_h = conv_out_size(input_h, k, s, p)\n",
    "            out_w = conv_out_size(input_w, k, s, p)\n",
    "            print(f\"Conv: kernel={k}, padding={p}, stride={s} -> out=({out_h}x{out_w})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b54541",
   "metadata": {},
   "source": [
    "\n",
    "## Implementación de la CNN (parametrizable)\n",
    "\n",
    "- **2+ capas convolucionales** con opciones de `kernel_size`, `stride`, `padding`.  \n",
    "- **Pooling** configurable.  \n",
    "- Capa fully-connected para clasificación (25 clases en `train`: letras A–Y excepto J y Z).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09379079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = len(np.unique(labels))\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 c1_out=32, c2_out=64, \n",
    "                 k1=3, s1=1, p1=1,\n",
    "                 k2=3, s2=1, p2=1,\n",
    "                 pool_kernel=2, pool_stride=2,\n",
    "                 use_pool=True):\n",
    "        super().__init__()\n",
    "        self.use_pool = use_pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, c1_out, kernel_size=k1, stride=s1, padding=p1)\n",
    "        self.conv2 = nn.Conv2d(c1_out, c2_out, kernel_size=k2, stride=s2, padding=p2)\n",
    "        self.pool  = nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride)\n",
    "\n",
    "        # Calcular tamaño tras conv/pool para definir la FC\n",
    "        h = w = 28\n",
    "        h = conv_out_size(h, k1, s1, p1)\n",
    "        w = conv_out_size(w, k1, s1, p1)\n",
    "        if use_pool:\n",
    "            h = pool_out_size(h, pool_kernel, pool_stride)\n",
    "            w = pool_out_size(w, pool_kernel, pool_stride)\n",
    "\n",
    "        h = conv_out_size(h, k2, s2, p2)\n",
    "        w = conv_out_size(w, k2, s2, p2)\n",
    "        if use_pool:\n",
    "            h = pool_out_size(h, pool_kernel, pool_stride)\n",
    "            w = pool_out_size(w, pool_kernel, pool_stride)\n",
    "\n",
    "        feat_dim = c2_out * h * w\n",
    "        self.head = nn.Linear(feat_dim, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        if self.use_pool:\n",
    "            x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if self.use_pool:\n",
    "            x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# Ejemplo de instanciación (padding=1, stride=1, kernel=3; pooling 2x2)\n",
    "model = SimpleCNN(k1=3, s1=1, p1=1, k2=3, s2=1, p2=1, pool_kernel=2, pool_stride=2, use_pool=True).to(DEVICE)\n",
    "sum(p.numel() for p in model.parameters()), model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddaa05b",
   "metadata": {},
   "source": [
    "\n",
    "## Entrenamiento (≥ 5 épocas) y evaluación (pérdida y precisión)\n",
    "\n",
    "- Optimizador **Adam**.  \n",
    "- Reporte de **loss** y **accuracy** en **train** y **valid** por época.  \n",
    "- Ajusta `EPOCHS`, `lr` y la configuración de la CNN para experimentar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89361628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_from_logits(logits, y_true):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == y_true).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss, running_acc, n = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bsz = yb.size(0)\n",
    "        running_loss += loss.item() * bsz\n",
    "        running_acc  += accuracy_from_logits(logits, yb) * bsz\n",
    "        n += bsz\n",
    "    return running_loss / n, running_acc / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    running_loss, running_acc, n = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        bsz = yb.size(0)\n",
    "        running_loss += loss.item() * bsz\n",
    "        running_acc  += accuracy_from_logits(logits, yb) * bsz\n",
    "        n += bsz\n",
    "    return running_loss / n, running_acc / n\n",
    "\n",
    "EPOCHS = 5\n",
    "lr = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
    "    va_loss, va_acc = evaluate(model, val_loader, loss_fn, DEVICE)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss)\n",
    "    history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {tr_loss:.4f}  Acc: {tr_acc:.4f} | \"\n",
    "          f\"Val Loss: {va_loss:.4f}  Acc: {va_acc:.4f}\")\n",
    "elapsed = time.time() - start\n",
    "print(f\"Tiempo total de entrenamiento: {elapsed/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23703686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Curvas de pérdida y precisión\n",
    "epochs = range(1, EPOCHS+1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"],   label=\"Val Loss\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.title(\"Curva de pérdida\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(epochs, history[\"val_acc\"],   label=\"Val Acc\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.title(\"Curva de precisión\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f94ef9",
   "metadata": {},
   "source": [
    "\n",
    "## (Opcional) Barrido de configuraciones de la CNN\n",
    "\n",
    "Ejemplo de cómo probar distintas combinaciones de `kernel_size`, `stride`, `padding` y `pooling` de forma breve (1–2 épocas)  \n",
    "para **comparar tamaños/precisiones** sin entrenar largo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026eb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configs = [\n",
    "    dict(k1=3,s1=1,p1=1,k2=3,s2=1,p2=1, pool_kernel=2, pool_stride=2, use_pool=True),\n",
    "    dict(k1=5,s1=1,p1=2,k2=3,s2=1,p2=1, pool_kernel=2, pool_stride=2, use_pool=True),\n",
    "    dict(k1=3,s1=2,p1=1,k2=3,s2=2,p2=1, pool_kernel=2, pool_stride=2, use_pool=False),\n",
    "    dict(k1=5,s1=2,p1=2,k2=5,s2=1,p2=2, pool_kernel=4, pool_stride=4, use_pool=True),\n",
    "]\n",
    "\n",
    "quick_results = []\n",
    "\n",
    "for cfg in configs:\n",
    "    m = SimpleCNN(**cfg).to(DEVICE)\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "    lf  = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenamiento rápido (1 época para demo)\n",
    "    tr_loss, tr_acc = train_one_epoch(m, train_loader, opt, lf, DEVICE)\n",
    "    va_loss, va_acc = evaluate(m, val_loader, lf, DEVICE)\n",
    "\n",
    "    # calcular tamaño de salida del extractor de características para documentar\n",
    "    h = w = 28\n",
    "    h = (h + 2*cfg[\"p1\"] - cfg[\"k1\"])//cfg[\"s1\"] + 1\n",
    "    w = (w + 2*cfg[\"p1\"] - cfg[\"k1\"])//cfg[\"s1\"] + 1\n",
    "    if cfg[\"use_pool\"]:\n",
    "        h = (h - cfg[\"pool_kernel\"])//cfg[\"pool_stride\"] + 1\n",
    "        w = (w - cfg[\"pool_kernel\"])//cfg[\"pool_stride\"] + 1\n",
    "    h = (h + 2*cfg[\"p2\"] - cfg[\"k2\"])//cfg[\"s2\"] + 1\n",
    "    w = (w + 2*cfg[\"p2\"] - cfg[\"k2\"])//cfg[\"s2\"] + 1\n",
    "    if cfg[\"use_pool\"]:\n",
    "        h = (h - cfg[\"pool_kernel\"])//cfg[\"pool_stride\"] + 1\n",
    "        w = (w - cfg[\"pool_kernel\"])//cfg[\"pool_stride\"] + 1\n",
    "\n",
    "    feat_dim = cfg[\"c2_out\"] if \"c2_out\" in cfg else 64\n",
    "    feat_dim *= h*w\n",
    "\n",
    "    quick_results.append({\n",
    "        \"cfg\": cfg,\n",
    "        \"train_loss\": tr_loss, \"train_acc\": tr_acc,\n",
    "        \"val_loss\": va_loss, \"val_acc\": va_acc,\n",
    "        \"feat_HxW\": (h, w), \"feat_dim\": feat_dim\n",
    "    })\n",
    "\n",
    "quick_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d018c",
   "metadata": {},
   "source": [
    "\n",
    "## Notas para evitar sobreajuste (overfitting)\n",
    "\n",
    "- Usar **data augmentation** (ya aplicado).  \n",
    "- Añadir **Dropout** entre capas fully-connected o tras convoluciones.  \n",
    "- Ajustar **weight decay** en Adam (`Adam(..., weight_decay=1e-4)`).  \n",
    "- Usar **early stopping** con validación o reducir la capacidad de la red.  \n",
    "- Aumentar el tamaño del conjunto de datos de entrenamiento.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningEnv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
