{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805a4481",
   "metadata": {},
   "source": [
    "\n",
    "# TMDB Movie Genre Classification (PyTorch + GloVe + LSTM)\n",
    "\n",
    "**Goal:** Predict the **main genre** from a movie *overview* using:\n",
    "- Clean text preprocessing (lowercase, regex, simple tokenization, basic stopwords)\n",
    "- **GloVe** pretrained word embeddings (100d, `glove-wiki-gigaword-100` via Gensim)\n",
    "- A simple **LSTM** classifier (Embedding → LSTM → Dropout → Dense → Softmax)\n",
    "- **Optimizations:** class weighting (to address imbalance), higher dropout\n",
    "- **Evaluation:** precision, recall, F1-score and accuracy (per-class report)\n",
    "\n",
    "> Minimal, readable, and didactic. No TensorFlow used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66c753",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup (optional)\n",
    "If you need to install dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b425b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies (uncomment if needed)\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# pip install pandas numpy scikit-learn gensim matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25501539",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8920861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.9.0-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.24.0-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.9.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\idb0227\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached torch-2.9.0-cp311-cp311-win_amd64.whl (109.3 MB)\n",
      "Using cached torchvision-0.24.0-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "Using cached torchaudio-2.9.0-cp311-cp311-win_amd64.whl (664 kB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\idb0227\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\torch\\\\include\\\\ATen\\\\native\\\\transformers\\\\cuda\\\\mem_eff_attention\\\\iterators\\\\predicated_tile_access_iterator_residual_last.h'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3363d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclass_weight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapi\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:427\u001b[39m\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    426\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    431\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _C: No se puede encontrar el módulo especificado."
     ]
    }
   ],
   "source": [
    "\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56dc5a9",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load data and extract the **main genre**\n",
    "We keep only the `overview` and the first genre in the `genres` JSON list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to CSV \n",
    "CSV_PATH = \"tmdb_5000_movies.csv\"  \n",
    "\n",
    "df = pd.read_csv(CSV_PATH, encoding='utf-8')\n",
    "\n",
    "df = df[['title', 'overview', 'genres']]\n",
    "\n",
    "def first_genre(genres_json):\n",
    "    try:\n",
    "        items = ast.literal_eval(genres_json)\n",
    "        if isinstance(items, list) and len(items) > 0 and 'name' in items[0]:\n",
    "            return items[0]['name']\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.nan\n",
    "\n",
    "df['main_genre'] = df['genres'].apply(first_genre)\n",
    "df = df.dropna(subset=['overview', 'main_genre']).reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe36048",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Text preprocessing\n",
    "Lowercase, regex-based tokenization (letters only), and a tiny custom stopword list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STOPWORDS = {\n",
    "    'the','a','an','and','or','if','in','on','of','for','to','from','by','with','at','as','is','are','was','were',\n",
    "    'be','been','being','this','that','it','its','into','about','over','after','before','between','among','because',\n",
    "    'but','so','than','too','very','can','could','should','would','may','might','will','just','do','does','did','doing',\n",
    "    'up','down','out','off','not','no','nor','also','such','their','there','then','when','where','who','whom',\n",
    "    'what','which','while','how','more','most','least','few','many'\n",
    "}\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[a-z]+\")\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    text = str(text).lower()\n",
    "    tokens = TOKEN_RE.findall(text)\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['overview'].apply(preprocess_text)\n",
    "df[['title','main_genre','tokens']].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2e02d",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Vocabulary\n",
    "We keep a capped vocabulary by frequency.  \n",
    "**Note:** We reserve index **0 for PAD**. No UNK is included in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_VOCAB = 20000\n",
    "PAD = \"<pad>\"\n",
    "\n",
    "# Count global frequencies\n",
    "freq = Counter(token for toks in df['tokens'] for token in toks)\n",
    "most_common = [w for w,_ in freq.most_common(MAX_VOCAB - 1)]  # -1 because index 0 is PAD\n",
    "\n",
    "# Build itos/stoi: index 0 is PAD, words start at 1\n",
    "itos = [PAD] + most_common\n",
    "stoi = {w:i for i,w in enumerate(itos)}  # PAD -> 0\n",
    "\n",
    "len(itos), itos[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e28b8",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Encode tokens → IDs (no PAD/UNK here)\n",
    "The `encode` function returns **only known token IDs** (words in the vocab).  \n",
    "Out-of-vocab tokens are **skipped**. Padding is **not** done here; it will be handled later in the `collate_fn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f52fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode(tokens):\n",
    "    ids = [stoi[t] for t in tokens if t in stoi and t != PAD]\n",
    "    return ids\n",
    "\n",
    "df['input_ids'] = df['tokens'].apply(encode)\n",
    "# Edge case: if any is empty, it will be handled by collate_fn (we'll pad to [0])\n",
    "empty_count = sum(1 for x in df['input_ids'] if len(x)==0)\n",
    "print(\"Empty sequences (will be padded to [0] on the fly):\", empty_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60265d5a",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Load GloVe (100d) and build the embedding matrix\n",
    "Row 0 (PAD) is all zeros. Words without a GloVe vector get a small random vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a77927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove = api.load(\"glove-wiki-gigaword-100\")  # 100-dim embeddings\n",
    "EMB_DIM = glove.vector_size\n",
    "\n",
    "emb_matrix = np.zeros((len(itos), EMB_DIM), dtype=np.float32)\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "for i, w in enumerate(itos):\n",
    "    if i == 0:  # PAD\n",
    "        emb_matrix[i] = np.zeros(EMB_DIM)\n",
    "    else:\n",
    "        vec = glove.get(w)\n",
    "        if vec is not None:\n",
    "            emb_matrix[i] = vec\n",
    "        else:\n",
    "            emb_matrix[i] = rng.normal(0, 0.6, EMB_DIM)\n",
    "\n",
    "emb_matrix = torch.tensor(emb_matrix)\n",
    "emb_matrix.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db25cb2",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Labels and train/validation split\n",
    "We will stratify by the main genre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genres = sorted(df['main_genre'].unique().tolist())\n",
    "label2id = {g:i for i,g in enumerate(genres)}\n",
    "id2label = {i:g for g,i in label2id.items()}\n",
    "\n",
    "df['label'] = df['main_genre'].map(label2id)\n",
    "\n",
    "X_list = df['input_ids'].tolist()\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_list, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "len(genres), genres[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb22baad",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Dataset and DataLoaders\n",
    "We keep sequences as **variable-length lists**. Padding to the batch max-length is done in `collate_fn` with PAD index `0`.  \n",
    "We also compute sequence lengths for efficient packing in the LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, seqs, labels):\n",
    "        self.seqs = seqs\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], int(self.labels[idx])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: list of (seq_ids_list, label)\n",
    "    seqs, labels = zip(*batch)\n",
    "    # handle empty sequences -> replace with [0] (PAD only)\n",
    "    seqs = [s if len(s)>0 else [0] for s in seqs]\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "\n",
    "    max_len = max(lengths).item()\n",
    "    padded = torch.zeros((len(seqs), max_len), dtype=torch.long)  # PAD=0\n",
    "    for i, s in enumerate(seqs):\n",
    "        padded[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded, lengths, labels\n",
    "\n",
    "train_ds = MovieDataset(X_train, y_train)\n",
    "val_ds   = MovieDataset(X_val, y_val)\n",
    "\n",
    "BATCH_SIZE_INIT = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE_INIT, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE_INIT, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f403aac",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Model (Embedding → LSTM → Dropout → Dense)\n",
    "We use `pack_padded_sequence` to efficiently handle variable-length sequences.  \n",
    "We take the **last hidden state** as the sequence representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1067c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size=128, num_classes=10, dropout=0.5, bidirectional=False):\n",
    "        super().__init__()\n",
    "        num_embeddings, emb_dim = emb_matrix.shape\n",
    "        self.embedding = nn.Embedding(num_embeddings, emb_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(emb_matrix)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        feat_dim = hidden_size * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(feat_dim, 128)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, lengths):\n",
    "        # input_ids: (B, T), lengths: (B,)\n",
    "        x = self.embedding(input_ids)  # (B, T, E)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h, c) = self.lstm(packed)\n",
    "        if self.lstm.bidirectional:\n",
    "            h_last = torch.cat([h[-2], h[-1]], dim=1)\n",
    "        else:\n",
    "            h_last = h[-1]\n",
    "        z = self.dropout(h_last)\n",
    "        z = self.act(self.fc1(z))\n",
    "        z = self.dropout(z)\n",
    "        logits = self.fc_out(z)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5224a10",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Train/Eval helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d111aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for Xb, Lb, yb in loader:\n",
    "        Xb, Lb, yb = Xb.to(device), Lb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb, Lb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    preds, golds = [], []\n",
    "    for Xb, Lb, yb in loader:\n",
    "        Xb, Lb, yb = Xb.to(device), Lb.to(device), yb.to(device)\n",
    "        logits = model(Xb, Lb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        preds.extend(pred)\n",
    "        golds.extend(yb.cpu().numpy().tolist())\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss, preds, golds\n",
    "\n",
    "def report(golds, preds, id2label):\n",
    "    target_names = [id2label[i] for i in sorted(set(golds))]\n",
    "    print(classification_report(golds, preds, target_names=target_names, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4469ac",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Baseline training (no class weights, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32840cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = len(genres)\n",
    "\n",
    "baseline = LSTMClassifier(\n",
    "    emb_matrix=emb_matrix,\n",
    "    hidden_size=128,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.2,\n",
    "    bidirectional=False\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(baseline.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_INIT = 5  # increase for better results if you want\n",
    "for epoch in range(1, EPOCHS_INIT+1):\n",
    "    tr_loss = train_one_epoch(baseline, train_dl, optimizer, criterion)\n",
    "    va_loss, va_preds, va_golds = evaluate(baseline, val_dl, criterion)\n",
    "    print(f\"[Baseline] Epoch {epoch}/{EPOCHS_INIT} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f}\")\n",
    "\n",
    "print(\"\\n[Baseline] Validation report:\")\n",
    "report(va_golds, va_preds, id2label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e050e",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Optimized training (class weights + dropout=0.5)\n",
    "- **Class weighting** to handle label imbalance.\n",
    "- Higher **dropout** for regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(\"Class weights:\", class_weights.cpu().numpy())\n",
    "\n",
    "optimized = LSTMClassifier(\n",
    "    emb_matrix=emb_matrix,\n",
    "    hidden_size=128,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.5,\n",
    "    bidirectional=False\n",
    ").to(device)\n",
    "\n",
    "optimizer_opt = torch.optim.Adam(optimized.parameters(), lr=1e-3)\n",
    "criterion_opt = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "EPOCHS_OPT = 8  # increase if you want\n",
    "for epoch in range(1, EPOCHS_OPT+1):\n",
    "    tr_loss = train_one_epoch(optimized, train_dl, optimizer_opt, criterion_opt)\n",
    "    va_loss, va_preds, va_golds = evaluate(optimized, val_dl, criterion_opt)\n",
    "    print(f\"[Optimized] Epoch {epoch}/{EPOCHS_OPT} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f}\")\n",
    "\n",
    "print(\"\\n[Optimized] Validation report:\")\n",
    "report(va_golds, va_preds, id2label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0458a",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Quick comparison\n",
    "Macro-F1, Precision, Recall, Accuracy (baseline vs optimized).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def quick_scores(golds, preds):\n",
    "    acc = accuracy_score(golds, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(golds, preds, average='macro', zero_division=0)\n",
    "    return {\"acc\":acc, \"precision_macro\":p, \"recall_macro\":r, \"f1_macro\":f1}\n",
    "\n",
    "# Re-evaluate to ensure we have the latest predictions\n",
    "_, base_preds, base_golds = evaluate(baseline, val_dl, nn.CrossEntropyLoss())\n",
    "_, opt_preds,  opt_golds  = evaluate(optimized, val_dl, nn.CrossEntropyLoss(weight=class_weights))\n",
    "\n",
    "print(\"Baseline:\", quick_scores(base_golds, base_preds))\n",
    "print(\"Optimized:\", quick_scores(opt_golds, opt_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1d0c7",
   "metadata": {},
   "source": [
    "\n",
    "## Notes\n",
    "- **Preprocessing:** minimal on purpose; extend with lemmatization/stemming if needed.\n",
    "- **GloVe:** we align embeddings to our vocab. PAD (0) is all zeros; missing words get small random vectors.\n",
    "- **No UNK in `encode`:** OOV tokens are simply skipped.\n",
    "- **Variable lengths:** we pad **only in the collate function** and use packing in the LSTM.\n",
    "- **Optimizations:** class weights, higher dropout. Try `bidirectional=True` or larger `hidden_size` for more capacity.\n",
    "- **Reproducibility:** for full determinism you can set random seeds and disable cuDNN variability (not shown to keep it concise).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_afectados)",
   "language": "python",
   "name": "venv_afectados"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
