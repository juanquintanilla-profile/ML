{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c06fb4",
   "metadata": {},
   "source": [
    "## Despliegue de la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1b213",
   "metadata": {},
   "source": [
    "Ejecutar en terminal con docker instalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker pull pgvector/pgvector:0.8.0-pg17\n",
    "\n",
    "docker run -e POSTGRES_USER=myuser            -e POSTGRES_PASSWORD=mypassword            -e POSTGRES_DB=mydatabase            --name my_postgres            -p 5432:5432            -d pgvector/pgvector:0.8.0-pg17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d204b",
   "metadata": {},
   "source": [
    "## Inicialización de la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e4daa",
   "metadata": {},
   "source": [
    "En primer lugar, inicializamos la DB postgres y añadimos la extensión para vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf6a1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to your database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mydatabase\",\n",
    "    user=\"myuser\",\n",
    "    password=\"mypassword\",\n",
    "    host=\"localhost\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e5caf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed extensions: [(13569, 'plpgsql', 10, 11, False, '1.0', None, None), (16385, 'vector', 10, 2200, True, '0.8.0', None, None)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create the 'vector' extension if it doesn't exist\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    conn.commit()\n",
    "\n",
    "# Verify the extension is installed\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT * FROM pg_extension;\")\n",
    "    extensions = cur.fetchall()\n",
    "    print(\"Installed extensions:\", extensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16866514",
   "metadata": {},
   "source": [
    "Conectamos con la DB y creamos una tabla \"chunks\" donde vamos a almacenar los fragmentos del texto\n",
    "Como vamos a usar los vectores de openai cuya dimensión es 3072, lo configuramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a table with a vector column\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE chunks (\n",
    "            id serial PRIMARY KEY,\n",
    "            texto_original TEXT,\n",
    "            empresa TEXT,\n",
    "            numero_pagina INTEGER,\n",
    "            embedding vector(3072)\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220b694",
   "metadata": {},
   "source": [
    "## Extracción e Ingesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44149140",
   "metadata": {},
   "source": [
    "### Llamamos extracción al proceso de leer de los pdfs y transformar la información a lenguaje natural.\n",
    "### Llamamos ingesta a vectorizar los documentos según una estrategia de troceado y a insertarlos tras su vectorización en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012dbc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pypdf \n",
    "%pip install langchain-community\n",
    "%pip install tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28b02d",
   "metadata": {},
   "source": [
    "Para este ejemplo vamos a usar PyPDF para la extracción y langchain para simplificar las llamadas. Al hacerlo asíncrono lo hacemos mucho mas rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "loader = PyPDFLoader(\"jga25-cuentas-anuales-consolidadas-2024.pdf\")\n",
    "pages = []\n",
    "async for page in tqdm(loader.alazy_load(), desc=\"Loading pages\"):\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54760a",
   "metadata": {},
   "source": [
    "Vamos a convertir el formato en un objeto similar a los campos de nuestra BD, tambien limpiamos algunas páginas que no tienen texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pagina:\n",
    "    def __init__(self, texto_fuente, pagina):\n",
    "        self.texto_fuente = texto_fuente\n",
    "        self.empresa = \"Iberdrola\"\n",
    "        self.pagina = pagina\n",
    "        self.vector = []\n",
    "    def __str__(self):\n",
    "        return f\"Pagina {self.pagina}: {self.texto_fuente}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12204eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the array of pages into an array of Pagina objects, excluding empty page_content\n",
    "paginas = [Pagina(page.page_content, page.metadata['page']) for page in pages if page.page_content]\n",
    "\n",
    "# Print the third Pagina object\n",
    "print(paginas[90])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e02864",
   "metadata": {},
   "source": [
    "## Ingesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba916d7",
   "metadata": {},
   "source": [
    "Instalamos el paquete langchain openai para hacer la vectorización de los pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd9133a",
   "metadata": {},
   "source": [
    "Desplegamos un modelo en azure openai de embeddings y lo configuramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ae460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    azure_endpoint=\"\",\n",
    "    api_key=\"\",\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbd85b",
   "metadata": {},
   "source": [
    "Vectorizamos la pagina 15 y lo imprimimos para comprobar que funciona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(paginas[15].texto_fuente)\n",
    "print(paginas[15], vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa822b4",
   "metadata": {},
   "source": [
    "Vectorizamos todas las paginas y guardamos el vector resultante en el objeto que ya habiamos creado, usamos tqdm para mostrar el progreso\n",
    "Si tuvieramos una gran cantidad de documentos, sería muy recomendable hacer el proceso asíncrono y paralelizarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7050c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "for pagina in tqdm(paginas, desc=\"Vectorizando paginas\"):\n",
    "    pagina.vector = embeddings.embed_query(pagina.texto_fuente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bc7a3",
   "metadata": {},
   "source": [
    "Una vez vectorizados, hacemos la inserción en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa278c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    for pagina in paginas:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO chunks (texto_original, empresa, numero_pagina, embedding)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", (pagina.texto_fuente, pagina.empresa, pagina.pagina, pagina.vector))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30afd6b",
   "metadata": {},
   "source": [
    "Comprobamos que el dato quedó bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT * FROM chunks;\")\n",
    "    rows = cur.fetchone()\n",
    "    print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daddc36",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "Conocemos retrieval como el proceso de busqueda de la información relevante, esta información es la que se insertará en un prompt para que el LLM pueda dar respuestas contextualizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pedimos al usuario una pregunta y la vectorizamos\n",
    "consulta = input(\"Introduce tu consulta: \")\n",
    "vector = embeddings.embed_query(consulta)\n",
    "vector = str(vector)  \n",
    "#Hacemos un select para buscar las paginas que tengan una similitud con la consulta\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT texto_original FROM chunks ORDER BY embedding <=> %s LIMIT 5;\", (vector,))\n",
    "    rows = cur.fetchall()\n",
    "    print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bf43a",
   "metadata": {},
   "source": [
    "Vamos a probarlo junto con un modelo de IA generativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciamos un modelo de chat de Azure OpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4.1\",\n",
    "    azure_endpoint=\"\",\n",
    "    api_key=\"\",\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pedimos al usuario una pregunta y la vectorizamos\n",
    "consulta = input(\"Introduce tu consulta: \")\n",
    "vector = embeddings.embed_query(consulta)\n",
    "vector = str(vector)  \n",
    "#Hacemos un select para buscar las paginas que tengan una similitud con la consulta\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT texto_original FROM chunks ORDER BY embedding <=> %s LIMIT 5;\", (vector,))\n",
    "    rows = cur.fetchall()\n",
    "    print(rows)\n",
    "#Hagamos una prueba insertando el resultado de la query en un prompt \n",
    "prompt = f\"\"\"\n",
    "Eres un experto en finanzas y contabilidad. Tu tarea es analizar el siguiente texto:\n",
    "{rows}\n",
    "\n",
    "Y responder a la siguiente pregunta:\n",
    "{consulta}\n",
    "\n",
    "El usuario no es experto en finanzas y contabilidad, por lo que tu respuesta debe ser clara y concisa.\n",
    "Distingue claramente entre los datos extraidos del texto y cualquier otra información que utilices para responder la pregunta.\n",
    "\"\"\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec6fc4",
   "metadata": {},
   "source": [
    "### Vamos a profundizar más en lo que estamos haciendo. Añadamos observabilidad para entender como funciona cada elemento del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772a17e",
   "metadata": {},
   "source": [
    "Añadimos un par de librerias para poder visualizar de manera cómoda que está pasando.\n",
    "Langsmith es el sistema de observabilidad de langchain\n",
    "Logfire es el sistema de observavilidad de pydantic\n",
    "Ambos tienen una versión gratuita, pero yo personalmente, prefiero logfire. Vamos a usar langsmith para generar las trazas y redirigirlas a logfire. \n",
    "Actualmente la mayoría de sistemas de observabilidad se basan en opentelemetry y son interoperables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfe99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install logfire\n",
    "%pip install langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279142a3",
   "metadata": {},
   "source": [
    "Configuramos logfire y las variables de OpenTelemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7e5b9",
   "metadata": {},
   "source": [
    "Nota: Si no recibe logs el sistema, puede ser porque los siguientes comandos conviene hacerlos antes de instanciar los llm con langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logfire\n",
    "\n",
    "os.environ['LANGSMITH_OTEL_ENABLED'] = 'true'\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "\n",
    "logfire.configure(token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b16629",
   "metadata": {},
   "source": [
    "Ampliamos la consulta anterior con el comando @traceable que genera trazas sobre la entrada y salida de cualquier función. Como hemos activado las variables de langsmith, todas las llamadas a llms se trazarán automáticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "#Vamos a dejar la consulta fija para que los experimentos sean mas faciles de comparar\n",
    "consulta = \"Facturacion según mercado\"\n",
    "vector = embeddings.embed_query(consulta)\n",
    "vector = str(vector)  \n",
    "#Con la etiqueta traceable, logfire nos permite ver el codigo de la funcion y los parametros que se le pasan\n",
    "@traceable\n",
    "def search_chunks(vector):\n",
    "    with conn.cursor() as cur:\n",
    "        #Añadimos un segundo parametro al select para poder medir la similitud.\n",
    "        cur.execute(\"SELECT texto_original,  1 - (embedding <=> %s) FROM chunks ORDER BY embedding <=> %s LIMIT 5;\", (vector,vector))\n",
    "        rows = cur.fetchall()\n",
    "        return(rows)\n",
    "\n",
    "def split_text(rows):\n",
    "    #discard the second column and return a combined string of the first column\n",
    "    return \"\\n\".join([row[0] for row in rows])\n",
    "\n",
    "#Hagamos una prueba insertando el resultado de la query en un prompt \n",
    "prompt = f\"\"\"\n",
    "Eres un experto en finanzas y contabilidad. Tu tarea es analizar el siguiente texto:\n",
    "{split_text(search_chunks(vector))}\n",
    "\n",
    "Y responder a la siguiente pregunta:\n",
    "{consulta}\n",
    "\n",
    "El usuario no es experto en finanzas y contabilidad, por lo que tu respuesta debe ser clara y concisa.\n",
    "Distingue claramente entre los datos extraidos del texto y cualquier otra información que utilices para responder la pregunta.\n",
    "\"\"\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23861495",
   "metadata": {},
   "source": [
    "Con N=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "#Vamos a dejar la consulta fija para que los experimentos sean mas faciles de comparar\n",
    "consulta = \"Facturacion según mercado\"\n",
    "vector = embeddings.embed_query(consulta)\n",
    "vector = str(vector)  \n",
    "#Con la etiqueta traceable, logfire nos permite ver el codigo de la funcion y los parametros que se le pasan\n",
    "@traceable\n",
    "def search_chunks(vector):\n",
    "    with conn.cursor() as cur:\n",
    "        #Añadimos un segundo parametro al select para poder medir la similitud.\n",
    "        cur.execute(\"SELECT texto_original,  1 - (embedding <=> %s) FROM chunks ORDER BY embedding <=> %s LIMIT 1;\", (vector,vector))\n",
    "        rows = cur.fetchall()\n",
    "        return(rows)\n",
    "\n",
    "def split_text(rows):\n",
    "    #discard the second column and return a combined string of the first column\n",
    "    return \"\\n\".join([row[0] for row in rows])\n",
    "\n",
    "#Hagamos una prueba insertando el resultado de la query en un prompt \n",
    "prompt = f\"\"\"\n",
    "Eres un experto en finanzas y contabilidad. Tu tarea es analizar el siguiente texto:\n",
    "{split_text(search_chunks(vector))}\n",
    "\n",
    "Y responder a la siguiente pregunta:\n",
    "{consulta}\n",
    "\n",
    "El usuario no es experto en finanzas y contabilidad, por lo que tu respuesta debe ser clara y concisa.\n",
    "Distingue claramente entre los datos extraidos del texto y cualquier otra información que utilices para responder la pregunta.\n",
    "\"\"\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af26f2d",
   "metadata": {},
   "source": [
    "Podemos ver que solo con una página, la respuesta es mucho menos interesante (carece de datos)\n",
    "Probemos ahora con N=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "#Vamos a dejar la consulta fija para que los experimentos sean mas faciles de comparar\n",
    "consulta = \"Facturacion según mercado\"\n",
    "vector = embeddings.embed_query(consulta)\n",
    "vector = str(vector)  \n",
    "#Con la etiqueta traceable, logfire nos permite ver el codigo de la funcion y los parametros que se le pasan\n",
    "@traceable\n",
    "def search_chunks(vector):\n",
    "    with conn.cursor() as cur:\n",
    "        #Añadimos un segundo parametro al select para poder medir la similitud.\n",
    "        cur.execute(\"SELECT texto_original,  1 - (embedding <=> %s) FROM chunks ORDER BY embedding <=> %s LIMIT 20;\", (vector,vector))\n",
    "        rows = cur.fetchall()\n",
    "        return(rows)\n",
    "\n",
    "def split_text(rows):\n",
    "    #discard the second column and return a combined string of the first column\n",
    "    return \"\\n\".join([row[0] for row in rows])\n",
    "\n",
    "#Hagamos una prueba insertando el resultado de la query en un prompt \n",
    "prompt = f\"\"\"\n",
    "Eres un experto en finanzas y contabilidad. Tu tarea es analizar el siguiente texto:\n",
    "{split_text(search_chunks(vector))}\n",
    "\n",
    "Y responder a la siguiente pregunta:\n",
    "{consulta}\n",
    "\n",
    "El usuario no es experto en finanzas y contabilidad, por lo que tu respuesta debe ser clara y concisa.\n",
    "Distingue claramente entre los datos extraidos del texto y cualquier otra información que utilices para responder la pregunta.\n",
    "\"\"\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f384fa",
   "metadata": {},
   "source": [
    "Esta respuesta parece la mejor hasta ahora, pero ha aumentado un ~30% el tiempo de ejecución y el coste ha sido de aproximadamente 0.03$, respecto a 0.006$ en el caso anterior y 0.01$ en el primer experimento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e123e",
   "metadata": {},
   "source": [
    "Otra consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "#Vamos a dejar la consulta fija para que los experimentos sean mas faciles de comparar\n",
    "consulta = \"Resumen de los estados financieros\"\n",
    "vector = embeddings.embed_query(consulta)\n",
    "vector = str(vector)  \n",
    "#Con la etiqueta traceable, logfire nos permite ver el codigo de la funcion y los parametros que se le pasan\n",
    "@traceable\n",
    "def search_chunks(vector):\n",
    "    with conn.cursor() as cur:\n",
    "        #Añadimos un segundo parametro al select para poder medir la similitud.\n",
    "        cur.execute(\"SELECT texto_original,  1 - (embedding <=> %s) FROM chunks ORDER BY embedding <=> %s LIMIT 20;\", (vector,vector))\n",
    "        rows = cur.fetchall()\n",
    "        return(rows)\n",
    "\n",
    "def split_text(rows):\n",
    "    #discard the second column and return a combined string of the first column\n",
    "    return \"\\n\".join([row[0] for row in rows])\n",
    "\n",
    "#Hagamos una prueba insertando el resultado de la query en un prompt \n",
    "prompt = f\"\"\"\n",
    "Eres un experto en finanzas y contabilidad. Tu tarea es analizar el siguiente texto:\n",
    "{split_text(search_chunks(vector))}\n",
    "\n",
    "Y responder a la siguiente pregunta:\n",
    "{consulta}\n",
    "\n",
    "El usuario no es experto en finanzas y contabilidad, por lo que tu respuesta debe ser clara y concisa.\n",
    "Distingue claramente entre los datos extraidos del texto y cualquier otra información que utilices para responder la pregunta.\n",
    "\"\"\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1a45f",
   "metadata": {},
   "source": [
    "Contesta razonablemente bien. Examinando la distancia del coseno entre las distintas respuestas, podemos sacar al menos dos conclusiones:\n",
    "- Entre la mejor y la peor no hay tanta diferencia, aunque el contenido es mucho mas util en los mejores resultados\n",
    "- El peor resultado es mas similar a la pregunta que el mejor resultado de la pregunta anterior\n",
    "Esto hace que no sea muy util implementar una estrategia de threshold (con un N mas grande, cortar todos los chunks cuya similitud < threshold ). Si lo pusieramos en 0.40, perderiamos todas las respuestas del caso anterior, si lo pusieramos en 0.20, no ganariamos nada..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d7f57",
   "metadata": {},
   "source": [
    "¿Te has fijado que hay algunos chunks un poco raros? (Pone sindatosindato...) tratemos de entender que está pasando.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2da15",
   "metadata": {},
   "source": [
    "## Mejorando la calidad del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c9443",
   "metadata": {},
   "source": [
    "Vamos a tratar de mejorar el retrieval usando un extractor mas avanzado\n",
    "El pdf que hemos usado tiene sobre todo tablas y algún que otro diagrama\n",
    "Vamos a intentar mejorar la conversión a texto. Algunos extractores interesantes son unestructured (propuesta de langchain) y llamaindex. Por simplificar vamos a usar la capacidad de openAI para interpretar pdfs que suele ser precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b23f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import tempfile\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"\",\n",
    ")\n",
    "def process_page(file_path: str, page_index: int) -> Pagina:\n",
    "    \"\"\"\n",
    "    Process a single page of a PDF file and return a Pagina object.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file to process\n",
    "        page_index (int): Index of the page to process (0-indexed)\n",
    "        \n",
    "    Returns:\n",
    "        Pagina: A Pagina object with the processed text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the PDF file\n",
    "    reader = PdfReader(file_path)\n",
    "    \n",
    "    # Create a temporary PDF file with the selected page\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False) as temp_pdf:\n",
    "        writer = PdfWriter()\n",
    "        writer.add_page(reader.pages[page_index])\n",
    "        writer.write(temp_pdf)\n",
    "        temp_pdf_path = temp_pdf.name\n",
    "    \n",
    "    # Encode the temporary PDF file in base64\n",
    "    with open(temp_pdf_path, \"rb\") as f:\n",
    "        base64_content = base64.b64encode(f.read()).decode('utf-8')\n",
    "        base64_content = f\"data:application/pdf;base64,{base64_content}\"\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    Eres un experto en el análisis de documentos de contabilidad.\n",
    "    Tu misión es analizar el documento de contabilidad y extraer toda la información para que pueda ser utilizada en un sistema de IA. Pon especial atención en las tablas.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_file\",\n",
    "                        \"filename\": f\"{file_path}_page_{page_index}.pdf\",\n",
    "                        \"file_data\": base64_content\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        reasoning={},\n",
    "        tools=[],\n",
    "        temperature=0,\n",
    "        max_output_tokens=20000,\n",
    "        top_p=1,\n",
    "        store=True\n",
    "    )\n",
    "    \n",
    "    # Create a Pagina object with the response text as texto_fuente\n",
    "    texto_fuente = response.output[0].content[0].text\n",
    "    pagina_obj = Pagina(texto_fuente=texto_fuente, pagina=page_index + 1)\n",
    "    \n",
    "    return pagina_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bce55d",
   "metadata": {},
   "source": [
    "Con el siguiente código, podemos extraer del PDF las páginas que queramos. Cada llamada tarda ~20 segundos, luego si tenemos 300 páginas tardaremos unos 100 minutos. Si tuvieramos que hacer este proceso de manera productiva, lo haríamos asíncrono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pypdf\n",
    "import pickle\n",
    "\n",
    "\n",
    "ruta_pdf = \"jga25-cuentas-anuales-consolidadas-2024.pdf\"\n",
    "with open(ruta_pdf, \"rb\") as file:\n",
    "    reader = pypdf.PdfReader(file)\n",
    "    num_pages = len(reader.pages)\n",
    "\n",
    "\n",
    "for i in tqdm(range(54, 56)): \n",
    "    pagina = process_page(ruta_pdf, i)\n",
    "    \n",
    "    # Save each page to a pickle file\n",
    "    with open(f\"pagina_{i}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(pagina, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7b5fa",
   "metadata": {},
   "source": [
    "Para ahorrarte la espera (y los tokens), te dejo el resultado en ficheros pickle en extracción_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddf1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ruta al directorio que contiene los archivos pickle\n",
    "ruta_directorio = \"extracción_v2\"\n",
    "\n",
    "# Lista para almacenar los objetos Pagina\n",
    "paginas = []\n",
    "\n",
    "# Leemos todos los archivos pickle en el directorio\n",
    "for archivo in os.listdir(ruta_directorio):\n",
    "    if archivo.endswith(\".pkl\"):\n",
    "        with open(os.path.join(ruta_directorio, archivo), \"rb\") as f:\n",
    "            pagina = pickle.load(f)\n",
    "            try:\n",
    "                paginas.append(pagina[0])\n",
    "            except:\n",
    "                paginas.append(pagina)  \n",
    "                print(f\"Error al cargar el archivo {pagina}\")\n",
    "\n",
    "# Imprimimos el número de páginas leídas\n",
    "print(f\"Se han leído {len(paginas)} páginas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9109c",
   "metadata": {},
   "source": [
    "Vectorizamos la nueva extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbae7b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizando paginas: 100%|██████████| 374/374 [02:47<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "for pagina in tqdm(paginas, desc=\"Vectorizando paginas\"):\n",
    "    pagina.vector = embeddings.embed_query(pagina.texto_fuente)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d439f4",
   "metadata": {},
   "source": [
    "Creamos una nueva colección para esta extracción mejorada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d9a0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a table with a vector column\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE chunksV2 (\n",
    "            id serial PRIMARY KEY,\n",
    "            texto_original TEXT,\n",
    "            empresa TEXT,\n",
    "            numero_pagina INTEGER,\n",
    "            embedding vector(3072)\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14133cd",
   "metadata": {},
   "source": [
    "Insertamos los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb2b1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    for pagina in paginas:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO chunksV2 (texto_original, empresa, numero_pagina, embedding)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", (pagina.texto_fuente, pagina.empresa, pagina.pagina, pagina.vector))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02e190",
   "metadata": {},
   "source": [
    "Vamos a probarlo con una consulta de las que hemos hecho antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3efe57f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aquí tienes una explicación sencilla sobre los **costes** según los datos extraídos del Informe financiero anual 2024 de Iberdrola, S.A. y sus sociedades dependientes.\n",
      "\n",
      "---\n",
      "\n",
      "## ¿Qué son los costes en este contexto?\n",
      "\n",
      "En el informe, los **costes** principalmente se refieren a:\n",
      "- **El valor de adquisición e inversión en los activos** de la empresa (por ejemplo, terrenos, edificios, instalaciones eléctricas, maquinaria, etc.).\n",
      "- **Gastos asociados al funcionamiento general del negocio**, incluyendo personal y otros.\n",
      "\n",
      "### 1. **Coste de los activos fijos**\n",
      "- **Activos fijos**: Son los bienes que utiliza la empresa durante varios años, como plantas eléctricas, flotas, edificios, terrenos, equipamiento, etc.\n",
      "\n",
      "**Datos extraídos (2023):**\n",
      "- El total del **coste de todos los activos** (sin descontar depreciaciones) era de **137.811 millones de euros** al inicio del año y aumentó a **140.326 millones de euros** al cierre de 2023.\n",
      "- Este aumento se debe principalmente a nuevas inversiones, ampliaciones de plantas y mejoras tecnológicas, menos las ventas o bajas de activos.\n",
      "\n",
      "**Ejemplos específicos de activos y su coste (valor final 2023):**\n",
      "- Terremos y construcciones: **3.041 millones de euros**\n",
      "- Centrales hidroeléctricas: **8.422 millones**\n",
      "- Centrales eólicas y otras renovables: **34.185 millones**\n",
      "- Instalaciones para distribución eléctrica: **39.514 millones**\n",
      "\n",
      "### 2. **Costes de personal**\n",
      "- **Gastos que paga la empresa por sus empleados:** sueldos, seguridad social, provisiones para pensiones, y otros gastos sociales.\n",
      "\n",
      "**Datos extraídos:**\n",
      "- **Total neto gastado en personal**: **2.994 millones de euros** en 2024 (**2.960 millones en 2023**).\n",
      "  - Esto incluye salarios y costes sociales, pero restando aquellos gastos que se suman al valor de los activos (como cuando el trabajo de los empleados se invierte en construir o mejorar una infraestructura).\n",
      "\n",
      "### 3. **Otros gastos y costes**\n",
      "- **Amortización y provisiones**: Representan el valor que pierde cada año el activo por el uso o deterioro (por ejemplo, máquinas que envejecen).\n",
      "  - En 2024, el importe de estas amortizaciones fue de **5.012 millones de euros**, un 6,5% mayor que el año anterior.\n",
      "- **Tributos**: impuestos que paga la empresa en los distintos países donde opera, por ejemplo, en 2024: **2.566 millones de euros** en total.\n",
      "\n",
      "---\n",
      "\n",
      "## Resumen fácil:\n",
      "\n",
      "- **Coste** = Lo que cuesta comprar, mantener y operar los bienes necesarios, como plantas eléctricas, y lo que se paga en salarios, impuestos y gastos asociados.\n",
      "- **Ejemplo sencillo:** Si Iberdrola compró una nueva planta eólica, el dinero invertido en esa compra y su construcción aparece como un coste de activo. El salario de los trabajadores y los impuestos relacionados también son costes.\n",
      "\n",
      "---\n",
      "\n",
      "## Nota adicional\n",
      "\n",
      "Toda la información y cifras de arriba están **extraídas directamente del informe anual 2024 de Iberdrola** (según el texto proporcionado), y he simplificado los conceptos para que sean entendibles sin conocimientos financieros avanzados.\n",
      "\n",
      "Si te interesa saber sobre un tipo concreto de coste o cómo afectan estos costes al resultado de la empresa, ¡dímelo y te lo explico!\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "#Vamos a dejar la consulta fija para que los experimentos sean mas faciles de comparar\n",
    "consulta = \"Hablame de los costes\"\n",
    "vector = embeddings.embed_query(consulta)\n",
    "vector = str(vector)  \n",
    "#Con la etiqueta traceable, logfire nos permite ver el codigo de la funcion y los parametros que se le pasan\n",
    "@traceable\n",
    "def search_chunks(vector):\n",
    "    with conn.cursor() as cur:\n",
    "        #Añadimos un segundo parametro al select para poder medir la similitud.\n",
    "        cur.execute(\"SELECT texto_original,  1 - (embedding <=> %s) FROM chunksV2 ORDER BY embedding <=> %s LIMIT 20;\", (vector,vector))\n",
    "        rows = cur.fetchall()\n",
    "        return(rows)\n",
    "\n",
    "def split_text(rows):\n",
    "    #discard the second column and return a combined string of the first column\n",
    "    return \"\\n\".join([row[0] for row in rows])\n",
    "\n",
    "#Hagamos una prueba insertando el resultado de la query en un prompt \n",
    "prompt = f\"\"\"\n",
    "Eres un experto en finanzas y contabilidad. Tu tarea es analizar el siguiente texto:\n",
    "{split_text(search_chunks(vector))}\n",
    "\n",
    "Y responder a la siguiente pregunta:\n",
    "{consulta}\n",
    "\n",
    "El usuario no es experto en finanzas y contabilidad, por lo que tu respuesta debe ser clara y concisa.\n",
    "Distingue claramente entre los datos extraidos del texto y cualquier otra información que utilices para responder la pregunta.\n",
    "\"\"\"\n",
    "\n",
    "answer = llm.invoke(prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da7701",
   "metadata": {},
   "source": [
    "Como puedes ver, la calidad de los datos tiene un impacto muy alto en la calidad del sistema final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
