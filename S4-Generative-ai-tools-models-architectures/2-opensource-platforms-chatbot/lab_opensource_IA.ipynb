{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Lab: Acceso a modelos open source de IA Generativa\n",
    "\n",
    "\n",
    "  ### Objetivos del Lab\n",
    "\n",
    "  - Aprender a usar \n",
    "    - LM Studio\n",
    "      - Como herramienta visual (GUI)\n",
    "      - Desde Python\n",
    "    - Ollama\n",
    "        - Desde la terminal (CLI)\n",
    "        - Desde Python\n",
    "\n",
    "  - Integrar un modelo local en nuestro Asistente financiero\n",
    "\n",
    "\n",
    "\n",
    "  ### Herramientas\n",
    "\n",
    "  - **LM Studio**: Interfaz gr√°fica\n",
    "\n",
    "  - **Ollama**: CLI para desarrolladores\n",
    "\n",
    "\n",
    "\n",
    "  ### Estructura del Lab\n",
    "\n",
    "  1. **Preparaci√≥n del entorno**\n",
    "\n",
    "  2. **LM Studio**\n",
    "\n",
    "  3. **Ollama**\n",
    "\n",
    "  4. **Modelos de Hugging Face desde Ollama**\n",
    "\n",
    "  5. **Comparaci√≥n: Local vs Cloud**\n",
    "\n",
    "  6. **Problema: Asistente Financiero Local**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 1. Preparaci√≥n del Entorno\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### Instalaci√≥n de dependencias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  Ejecuta la siguiente celda para instalar las librer√≠as necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias para modelos open source\n",
    "!pip install ollama lmstudio python-dotenv google-genai openai \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Instalaci√≥n de herramientas\n",
    "\n",
    "\n",
    "\n",
    "  #### LM Studio\n",
    "\n",
    "\n",
    "\n",
    "  1. Descarga desde [lmstudio.ai](https://lmstudio.ai/download)\n",
    "\n",
    "  2. Instala la aplicaci√≥n para tu sistema operativo\n",
    "\n",
    "  3. Ejecuta LM Studio\n",
    "\n",
    "\n",
    "\n",
    "  #### Ollama\n",
    "\n",
    "\n",
    "\n",
    "  1. Descarga desde [ollama.com](https://ollama.com/download)\n",
    "\n",
    "  2. Instala seg√∫n tu sistema operativo\n",
    "\n",
    "  3. Verifica instalaci√≥n ejecutando `ollama --version` en terminal\n",
    "\n",
    "\n",
    "\n",
    "  ### üîß Verificaci√≥n de instalaci√≥n\n",
    "\n",
    "\n",
    "\n",
    "  **LM Studio**: Abre la aplicaci√≥n y verifica que aparezca la interfaz\n",
    "\n",
    "  **Ollama**: Ejecuta `ollama --version` en terminal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 2. LM Studio - La Forma Visual\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### Caracter√≠sticas principales:\n",
    "\n",
    "\n",
    "\n",
    "  - **Interfaz gr√°fica intuitiva**: Ideal para principiantes\n",
    "\n",
    "  - **Gesti√≥n visual de modelos**: Descarga y organizaci√≥n sencilla\n",
    "\n",
    "  - **Servidor local integrado**: API compatible con OpenAI\n",
    "\n",
    "  - **Soporte multiplataforma**: Windows, macOS, Linux\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### Pasos para usar LM Studio:\n",
    "\n",
    "\n",
    "\n",
    "  1. **Abre LM Studio**\n",
    "\n",
    "\n",
    "\n",
    "  2. **Busca un modelo** (ej: \"qwen2.5:0.5b\")\n",
    "\n",
    "\n",
    "\n",
    "  3. **Descarga el modelo**\n",
    "\n",
    "\n",
    "\n",
    "  4. **Inicia el servidor local**\n",
    "\n",
    "\n",
    "\n",
    "  5. **Conecta desde Python**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  üí° **Consejo**: Empieza con modelos peque√±os como `qwen2.5:0.5b` para pruebas r√°pidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### üìö Documentaci√≥n √∫til:\n",
    "\n",
    "\n",
    "\n",
    "  - [LM Studio Official Site](https://lmstudio.ai/)\n",
    "\n",
    "\n",
    "  - [LM Studio Documentation](https://lmstudio.ai/docs/app)\n",
    "\n",
    "\n",
    "  - [LM Studio Python SDK](https://github.com/lmstudio-ai/lmstudio-python)\n",
    "\n",
    "\n",
    "\n",
    "  ### Opci√≥n 1: SDK Nativo de LM Studio\n",
    "\n",
    "\n",
    "\n",
    "  **Ventajas**: Funciones espec√≠ficas, streaming avanzado, gesti√≥n completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "import json\n",
    "import pprint\n",
    "from lmstudio import LlmPredictionConfig\n",
    "\n",
    "def usar_lm_studio_nativo(mensaje, temperatura=0.7):\n",
    "    \"\"\"Usar SDK nativo de LM Studio\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Obtener modelo \n",
    "        model = lms.llm(\"qwen2.5-0.5b-instruct\")\n",
    "        #client = lms.get_default_client()\n",
    "        #model = client.llm.load_new_instance(\"qwen2.5-0.5b-instruct\")  \n",
    "        \n",
    "        # 2. Crear contexto de chat\n",
    "        chat = lms.Chat(\"Eres un asistente √∫til y amigable.\")\n",
    "        chat.add_user_message(mensaje)\n",
    "\n",
    "        config = LlmPredictionConfig(\n",
    "            temperature=temperatura,\n",
    "            max_tokens=1\n",
    "        )\n",
    "        \n",
    "        # 3. Generar respuesta\n",
    "        result = model.respond(chat, config=config)\n",
    "        \n",
    "        print(\"üñ•Ô∏è LM Studio responde:\")\n",
    "        print(result.content)\n",
    "        \n",
    "        print(\"Stats:\")\n",
    "        print(result.stats)  # Mostrar detalles del resultado\n",
    "\n",
    "        \n",
    "        # 4. Informaci√≥n adicional\n",
    "        print(f\"\\nüìä Tokens generados: {result.stats.predicted_tokens_count}\")\n",
    "        print(f\"‚ö° Tiempo al primer token: {result.stats.time_to_first_token_sec:.2f}s\")\n",
    "        print(f\"üõë Raz√≥n de parada: {result.stats.stop_reason}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con SDK nativo de LM Studio: {e}\")\n",
    "        print(\"üí° Aseg√∫rate de que LM Studio est√© corriendo\")\n",
    "        return None\n",
    "\n",
    "def usar_lm_studio_streaming(mensaje):\n",
    "    \"\"\"Demostrar streaming con SDK nativo\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Obtener modelo\n",
    "        model = lms.llm(\"qwen2.5-0.5b-instruct\")\n",
    "        \n",
    "        # 2. Crear contexto\n",
    "        chat = lms.Chat(\"Eres un asistente √∫til y amigable.\")\n",
    "        chat.add_user_message(mensaje)\n",
    "        \n",
    "        # 3. Streaming\n",
    "        print(f\"üñ•Ô∏è LM Studio (streaming) responde:\")\n",
    "        print(\"üñ•Ô∏è Asistente: \", end=\"\", flush=True)\n",
    "        result = model.respond_stream(chat) \n",
    "        for fragment in result:\n",
    "            print(fragment.content, end=\"\", flush=True)\n",
    "        \n",
    "        print()  # Nueva l√≠nea al final\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con streaming: {e}\")\n",
    "\n",
    "# Probar SDK nativo\n",
    "#usar_lm_studio_nativo(\"¬øCu√°l es la capital de Espa√±a?\")\n",
    "usar_lm_studio_streaming(\"Explica brevemente qu√© es la inteligencia artificial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Opci√≥n 2: SDK de OpenAI\n",
    "\n",
    "\n",
    "\n",
    "  **Ventajas**: Est√°ndar de industria, c√≥digo reutilizable, compatible con todos los proveedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "def usar_lm_studio_openai(mensaje, temperatura=0.7):\n",
    "    \"\"\"Conectar con LM Studio usando SDK de OpenAI\"\"\"\n",
    "    \n",
    "    # 1. Configurar cliente para LM Studio\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:1234/v1\",  # Servidor local de LM Studio\n",
    "        api_key=\"\"  \n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 2. Hacer llamada est√°ndar\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"\",  # Nombre del modelo en LM Studio\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": mensaje}\n",
    "            ],\n",
    "            temperature=temperatura,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        # 3. Extraer respuesta\n",
    "        contenido = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"üñ•Ô∏è LM Studio responde:\")\n",
    "        print(contenido)\n",
    "        \n",
    "        # 4. Informaci√≥n adicional\n",
    "        print(f\"\\nüìä Tokens usados: {response.usage.total_tokens}\")\n",
    "        print(f\"üè† Ejecut√°ndose localmente - sin coste\")\n",
    "        \n",
    "        return contenido\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error conectando con LM Studio: {e}\")\n",
    "        print(\"üí° Aseg√∫rate de que LM Studio est√© corriendo con el servidor iniciado\")\n",
    "        return None\n",
    "\n",
    "# Probar conexi√≥n con LM Studio\n",
    "usar_lm_studio_openai(\"¬øCu√°l es la capital de Espa√±a?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Funciones √∫tiles del SDK Nativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "def listar_modelos_cargados():\n",
    "    \"\"\"Listar modelos actualmente cargados en memoria\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Listar todos los modelos cargados\n",
    "        todos_modelos = lms.list_loaded_models()\n",
    "        modelos_llm = lms.list_loaded_models(\"llm\")\n",
    "        modelos_embedding = lms.list_loaded_models(\"embedding\")\n",
    "        \n",
    "        print(\"üìã Modelos cargados en memoria:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not todos_modelos:\n",
    "            print(\"‚ùå No hay modelos cargados en memoria\")\n",
    "            print(\"üí° Carga un modelo desde la interfaz de LM Studio\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üîÑ Total de modelos cargados: {len(todos_modelos)}\")\n",
    "        print(f\"ü§ñ Modelos LLM: {len(modelos_llm)}\")\n",
    "        print(f\"üîç Modelos embedding: {len(modelos_embedding)}\")\n",
    "        print()\n",
    "        \n",
    "        # Mostrar detalles de cada modelo\n",
    "        for i, modelo in enumerate(todos_modelos, 1):\n",
    "            print(f\"{i}. {modelo}\")\n",
    "        \n",
    "        return {\n",
    "            'todos': todos_modelos,\n",
    "            'llm': modelos_llm,\n",
    "            'embedding': modelos_embedding\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listando modelos cargados: {e}\")\n",
    "        print(\"üí° Aseg√∫rate de que LM Studio est√© ejecut√°ndose\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar funci√≥n\n",
    "listar_modelos_cargados()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 2. Listar modelos descargados localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_modelos_descargados():\n",
    "    \"\"\"Listar modelos disponibles localmente en LM Studio\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Listar todos los modelos descargados\n",
    "        todos_descargados = lms.list_downloaded_models()\n",
    "        llm_descargados = lms.list_downloaded_models(\"llm\")\n",
    "        embedding_descargados = lms.list_downloaded_models(\"embedding\")\n",
    "        \n",
    "        print(\"üìã Modelos descargados localmente:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not todos_descargados:\n",
    "            print(\"‚ùå No hay modelos descargados\")\n",
    "            print(\"üí° Descarga modelos desde la interfaz de LM Studio\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üì¶ Total de modelos descargados: {len(todos_descargados)}\")\n",
    "        print(f\"ü§ñ Modelos LLM: {len(llm_descargados)}\")\n",
    "        print(f\"üîç Modelos embedding: {len(embedding_descargados)}\")\n",
    "        print()\n",
    "        \n",
    "        # Mostrar detalles de cada modelo descargado\n",
    "        for i, modelo in enumerate(todos_descargados, 1):\n",
    "            print(f\"{i}. {modelo}\")\n",
    "        \n",
    "        return {\n",
    "            'todos': todos_descargados,\n",
    "            'llm': llm_descargados,\n",
    "            'embedding': embedding_descargados\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listando modelos descargados: {e}\")\n",
    "        print(\"üí° Aseg√∫rate de que LM Studio est√© ejecut√°ndose\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar funci√≥n\n",
    "listar_modelos_descargados()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Comparaci√≥n: SDK Nativo vs OpenAI\n",
    "\n",
    "  - **SDK OpenAI**: Para m√°xima compatibilidad y migraci√≥n f√°cil\n",
    "\n",
    "  - **SDK Nativo**: Para funciones avanzadas espec√≠ficas de LM Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3. Ollama - La Forma CLI\n",
    "\n",
    "\n",
    "\n",
    "  ### Caracter√≠sticas principales:\n",
    "\n",
    "\n",
    "\n",
    "  - **Interfaz de l√≠nea de comandos**: Ideal para desarrolladores\n",
    "\n",
    "  - **Gesti√≥n eficiente de modelos**: Comandos simples y potentes\n",
    "\n",
    "  - **API REST integrada**: Compatible con OpenAI SDK\n",
    "\n",
    "\n",
    "\n",
    "  ### Pasos para usar Ollama:\n",
    "\n",
    "\n",
    "\n",
    "  1. **[Instalar Ollama](https://ollama.com/)**\n",
    "\n",
    "  2. **Descargar modelo**: `ollama run gemma3:1b`\n",
    "\n",
    "  3. **Verificar instalaci√≥n**: `ollama list`\n",
    "\n",
    "  4. **Conectar desde Python**\n",
    "\n",
    "\n",
    "  üìö **Documentaci√≥n √∫til:**\n",
    "\n",
    "  - [Ollama Official Site](https://ollama.com/)\n",
    "\n",
    "  - [Ollama Python SDK](https://github.com/ollama/ollama-python)\n",
    "\n",
    "  - [Ollama Model Library](https://ollama.com/library)\n",
    "\n",
    "  - [Parametros validos Python SDK](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  #### Opci√≥n 1: SDK Nativo de Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pprint\n",
    "\n",
    "def usar_ollama_nativo(mensaje, temperatura=0.7):\n",
    "    \"\"\"Conectar con Ollama usando SDK nativo\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Llamada simple y directa\n",
    "        response = ollama.chat(\n",
    "            model=\"gemma3:1b\", \n",
    "            messages=[\n",
    "                {'role': 'user', 'content': mensaje}\n",
    "            ],\n",
    "            options={\n",
    "                'temperature': temperatura,\n",
    "                'num_predict': 150\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 2. Extraer respuesta\n",
    "        contenido = response['message']['content']\n",
    "        \n",
    "        print(\"ü¶ô Ollama responde:\")\n",
    "        print(contenido)\n",
    "\n",
    "        print(\"\\nDetalles de la respuesta:\")\n",
    "        pprint.pprint(response.model_dump())\n",
    "        \n",
    "        # 3. Informaci√≥n adicional\n",
    "        if 'eval_count' in response:\n",
    "            print(f\"\\nüìä Tokens generados: {response['eval_count']}\")\n",
    "        if 'eval_duration' in response:\n",
    "            duracion = response['eval_duration'] / 1e9  # Convertir a segundos\n",
    "            print(f\"‚ö° Tiempo de evaluaci√≥n: {duracion:.2f}s\")\n",
    "        \n",
    "        return contenido\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con SDK nativo de Ollama: {e}\")\n",
    "\n",
    "def usar_ollama_streaming(mensaje):\n",
    "    \"\"\"Demostrar streaming con SDK nativo de Ollama\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"ü¶ô Ollama (streaming) responde:\")\n",
    "        print(\"ü¶ô Asistente: \", end=\"\", flush=True)\n",
    "        \n",
    "        # 1. Streaming nativo\n",
    "        stream = ollama.chat(\n",
    "            model=\"gemma3:1b\",\n",
    "            messages=[{'role': 'user', 'content': mensaje}],\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # 2. Procesar chunks\n",
    "        for chunk in stream:\n",
    "            if 'message' in chunk:\n",
    "                contenido = chunk['message']['content']\n",
    "                print(contenido, end=\"\", flush=True)\n",
    "        \n",
    "        print()  # Nueva l√≠nea al final\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con streaming: {e}\")\n",
    "\n",
    "# Probar funciones\n",
    "#usar_ollama_nativo(\"¬øCu√°l es la capital de Espa√±a?\")\n",
    "usar_ollama_streaming(\"Explica brevemente qu√© es la inteligencia artificial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### Opci√≥n 2: SDK de OpenAI\n",
    "\n",
    "\n",
    "\n",
    "  **Ventajas**: Est√°ndar de industria, c√≥digo reutilizable, compatible con todos los proveedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pprint\n",
    "\n",
    "\n",
    "def usar_ollama_con_openai_sdk(mensaje, temperatura=0.7):\n",
    "    \"\"\"Conectar con Ollama usando SDK de OpenAI\"\"\"\n",
    "    \n",
    "    # 1. Configurar cliente para Ollama\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",  # Puerto est√°ndar de Ollama\n",
    "        api_key=\"ollama\"  # API key dummy, Ollama no la requiere\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 2. Hacer llamada est√°ndar\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gemma3:1b\", \n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": mensaje}\n",
    "            ],\n",
    "            temperature=temperatura,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        # 3. Extraer respuesta\n",
    "        contenido = response.choices[0].message.content\n",
    "        \n",
    "        print(f\"ü¶ô Ollama responde:\")\n",
    "        print(contenido)\n",
    "\n",
    "        print(\"\\nDetalles de la respuesta:\")\n",
    "        pprint.pprint(response.model_dump())\n",
    "        \n",
    "        # 4. Informaci√≥n adicional\n",
    "        if hasattr(response, 'usage') and response.usage:\n",
    "            print(f\"\\nüìä Tokens usados: {response.usage.total_tokens}\")\n",
    "        print(f\"üè† Ejecut√°ndose localmente - sin coste\")\n",
    "        \n",
    "        return contenido\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error conectando con Ollama: {e}\")\n",
    "        return None\n",
    "\n",
    "# Probar conexi√≥n con Ollama\n",
    "usar_ollama_con_openai_sdk(\"¬øCu√°l es la capital de Espa√±a?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Funciones espec√≠ficas del SDK Nativo de Ollama\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### 1. Gesti√≥n de modelos (listar e inspeccionar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gestionar_modelos_ollama():\n",
    "    \"\"\"Listar modelos disponibles y mostrar informaci√≥n detallada\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Listar modelos disponibles\n",
    "        modelos = ollama.list()\n",
    "        \n",
    "        print(\"üìã Modelos disponibles en Ollama:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not modelos['models']:\n",
    "            print(\"‚ùå No hay modelos instalados\")\n",
    "            print(\"üí° Instala un modelo con: ollama run gemma2:2b\")\n",
    "            return None\n",
    "        \n",
    "        # Mostrar lista de modelos\n",
    "        for modelo in modelos['models']:\n",
    "            nombre = modelo['model']\n",
    "            tama√±o = modelo['size'] / (1024**3)  # Convertir a GB\n",
    "            modificado = modelo['modified_at']\n",
    "            \n",
    "            print(f\"{nombre} ({tama√±o:.1f} GB)\")\n",
    "            print(\"-\"*50)\n",
    "            print(f\"  üìÖ Modificado: {modificado}\")\n",
    "            print()\n",
    "        \n",
    "            # Obtener informaci√≥n espec√≠fica del modelo\n",
    "            info = ollama.show(modelo['model'])\n",
    "            \n",
    "            # Informaci√≥n b√°sica\n",
    "            if 'modelfile' in info:\n",
    "                print(\"üìã Modelfile:\")\n",
    "                print(info['modelfile'][:200] + \"...\" if len(info['modelfile']) > 200 else info['modelfile'])\n",
    "                print()\n",
    "            \n",
    "            # Par√°metros\n",
    "            if 'parameters' in info:\n",
    "                print(\"‚öôÔ∏è Par√°metros:\")\n",
    "                print(info['parameters'])\n",
    "                print()\n",
    "            \n",
    "            # Template\n",
    "            if 'template' in info:\n",
    "                print(\"üìù Template:\")\n",
    "                print(info['template'][:150] + \"...\" if len(info['template']) > 150 else info['template'])\n",
    "                print()\n",
    "        \n",
    "        return modelos\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ejemplos de uso\n",
    "gestionar_modelos_ollama()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 2. Descargar modelos program√°ticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descargar_modelo_si_no_existe(nombre_modelo):\n",
    "    \"\"\"Descargar un modelo solo si no existe\"\"\"\n",
    "    try:\n",
    "        # Verificar si el modelo ya existe\n",
    "        modelos = ollama.list()\n",
    "        modelos_instalados = [m['model'] for m in modelos['models']]\n",
    "        \n",
    "        if nombre_modelo in modelos_instalados:\n",
    "            print(f\"‚úÖ Modelo {nombre_modelo} ya est√° instalado\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"üì• Descargando modelo {nombre_modelo}...\")\n",
    "        print(\"‚è≥ Esto puede tomar varios minutos...\")\n",
    "        \n",
    "        # Descargar el modelo\n",
    "        ollama.pull(nombre_modelo)\n",
    "        \n",
    "        print(f\"‚úÖ Modelo {nombre_modelo} descargado correctamente\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error descargando modelo: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejemplo de uso (comentado para evitar descargas no deseadas)\n",
    "descargar_modelo_si_no_existe(\"qwen2.5:0.5b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 4. Modelos de Hugging Face en Ollama\n",
    "\n",
    "\n",
    "  ### üöÄ Integraci√≥n con Hugging Face\n",
    "\n",
    "  **Formato**: `ollama run hf.co/usuario/nombre-modelo-GGUF:cuantizaci√≥n`\n",
    "\n",
    "  **Buscar**: [Hugging Face Models](https://huggingface.co/models) ‚Üí Filtrar por \"GGUF\"\n",
    "\n",
    "\n",
    "  üìö **Documentaci√≥n √∫til:**\n",
    "\n",
    "\n",
    "  - [Hugging Face Models](https://huggingface.co/models)\n",
    "\n",
    "  - [GGUF Format Guide](https://huggingface.co/docs/transformers/main/gguf)\n",
    "\n",
    "\n",
    "\n",
    "  ### üîç Entendiendo las cuantizaciones\n",
    "\n",
    "  Las cuantizaciones siguen este patr√≥n: **Q[bits]_[m√©todo]_[variante]**\n",
    "\n",
    "  - **Q**: Indica que es una cuantizaci√≥n (Quantization)\n",
    "\n",
    "  - **N√∫mero**: Bits por peso (2, 4, 5, 6, 8) - menos bits = menor tama√±o\n",
    "\n",
    "  - **K**: M√©todo k-quantization\n",
    "\n",
    "  - **Sufijos**:\n",
    "\n",
    "    - **S**: Small (menor precisi√≥n, m√°s r√°pido)\n",
    "\n",
    "    - **M**: Medium (balance entre precisi√≥n y velocidad)\n",
    "\n",
    "    - **L**: Large (mayor precisi√≥n, m√°s lento)\n",
    "\n",
    "    - **_0**: Otra forma de cuantizar mas antigua\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  üí° **Consejo pr√°ctico**: Empieza con Q4_K_M para un buen balance\n",
    "\n",
    "  ### üéØ Cuantizaciones m√°s comunes\n",
    "\n",
    "  | Cuantizaci√≥n | Tama√±o | Calidad | Velocidad |\n",
    "  |-------------|--------|---------|-----------|\n",
    "  | **Q2_K** | Muy peque√±o | Baja | Muy r√°pida |\n",
    "  | **Q4_K_M** | Peque√±o | Buena | R√°pida |\n",
    "  | **Q5_K_M** | Medio | Muy buena | Media |\n",
    "  | **Q6_K** | Grande | Excelente | Lenta |\n",
    "  | **Q8_0** | Muy grande | Casi perfecta | Muy lenta |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usar_modelo_huggingface(mensaje, modelo_hf=\"hf.co/MaziyarPanahi/gemma-3-1b-it-GGUF:Q4_K_M\"):\n",
    "    \"\"\"Usar un modelo de Hugging Face en Ollama\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Verificar si el modelo ya est√° instalado\n",
    "        modelos = ollama.list()\n",
    "        modelos_instalados = [m['model'] for m in modelos['models']]\n",
    "        \n",
    "        if modelo_hf not in modelos_instalados:\n",
    "            print(f\"üì• Modelo {modelo_hf} no est√° instalado\")\n",
    "            print(f\"üí° Comando: ollama pull {modelo_hf}\")\n",
    "            print(\"üîÑ Ejecuta el comando en terminal y luego vuelve aqu√≠\")\n",
    "            return None\n",
    "        \n",
    "        # Usar el modelo de HF\n",
    "        response = ollama.chat(\n",
    "            model=modelo_hf,\n",
    "            messages=[{'role': 'user', 'content': mensaje}]\n",
    "        )\n",
    "        \n",
    "        print(f\"ü§ó Hugging Face responde:\")\n",
    "        print(response['message']['content'])\n",
    "        \n",
    "        return response['message']['content']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con modelo HF: {e}\")\n",
    "        print(f\"üí° Aseg√∫rate de descargar el modelo: ollama run {modelo_hf}\")\n",
    "        return None\n",
    "\n",
    "# Ejemplo de uso\n",
    "usar_modelo_huggingface(\"¬øCu√°l es la capital de Francia?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 5. Comparaci√≥n: Local vs Cloud\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### üéØ Objetivo de la comparaci√≥n\n",
    "\n",
    "\n",
    "\n",
    "  Comparar el rendimiento entre modelos locales y en la nube usando **el mismo modelo base** para obtener una comparaci√≥n justa.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### üîç Metodolog√≠a\n",
    "\n",
    "\n",
    "\n",
    "  1. **Mismo modelo**: Usar Gemma para ambos casos (local y nube)\n",
    "\n",
    "  2. **Misma pregunta**: Prompt id√©ntico para ambos modelos\n",
    "\n",
    "  3. **M√©tricas consistentes**: Tiempo de respuesta, caracteres, calidad\n",
    "\n",
    "  4. **Condiciones controladas**: Misma configuraci√≥n de temperatura y tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from google import genai    \n",
    "from google.genai import types\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "cliente_gemini = genai.Client()\n",
    "\n",
    "class EvaluacionCalidad(BaseModel):\n",
    "    \"\"\"Define la estructura para la evaluaci√≥n de calidad usando Pydantic.\"\"\"\n",
    "    razonamiento: str\n",
    "    puntuacion_calidad: int = Field(default=0, ge=0, le=100, description=\"Puntuaci√≥n entre 0 y 100\")\n",
    "\n",
    "\n",
    "def evaluar_calidad_respuesta(prompt, respuesta_generada):\n",
    "    \"\"\"\n",
    "    Utiliza Gemini 2.5 Flash con un esquema Pydantic para evaluar la calidad de una respuesta.\n",
    "    \"\"\"\n",
    "    if not cliente_gemini:\n",
    "        print(\"‚ùå  Cliente de Gemini no inicializado. Omitiendo evaluaci√≥n.\")\n",
    "        return {\"puntuacion_calidad\": 0, \"razonamiento\": \"Cliente no configurado.\"}\n",
    "        \n",
    "    print(\"\\nüîé  Evaluando calidad con Gemini 2.5 Flash\")\n",
    "    \n",
    "    evaluador_prompt = f\"\"\"\n",
    "    **Tarea de Evaluaci√≥n de Calidad de Respuesta de LLM**\n",
    "    **Contexto:**\n",
    "    - Prompt Original: \"{prompt}\"\n",
    "    - Respuesta a Evaluar: \"{respuesta_generada}\"\n",
    "    **Instrucciones:**\n",
    "    Act√∫a como un experto evaluador de IA. Analiza la \"Respuesta a Evaluar\" bas√°ndote en el \"Prompt Original\".\n",
    "    Eval√∫a seg√∫n: Relevancia, Coherencia, Precisi√≥n y Completitud.\n",
    "    Proporciona una puntuaci√≥n y un razonamiento breve.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Se utiliza el m√©todo client.models.generate_content como indicaste.\n",
    "        respuesta_evaluador = cliente_gemini.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=[types.Content(parts=[types.Part.from_text(text=evaluador_prompt)])],\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=EvaluacionCalidad,\n",
    "                temperature=0,\n",
    "            )\n",
    "        )\n",
    "        evaluacion = respuesta_evaluador.parsed \n",
    "        print(f\"‚úÖ  Evaluaci√≥n completada. Puntuaci√≥n: {evaluacion.puntuacion_calidad}\")\n",
    "        return evaluacion\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå  Error durante la evaluaci√≥n de calidad: {e}\")\n",
    "        return {\"puntuacion_calidad\": 0, \"razonamiento\": \"Error en la evaluaci√≥n.\"}\n",
    "\n",
    "\n",
    "def medir_y_evaluar(funcion_generadora, nombre_proveedor, prompt):\n",
    "    \"\"\"\n",
    "    Mide el rendimiento de una funci√≥n generadora y eval√∫a la calidad de su respuesta.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä  PROBANDO: {nombre_proveedor.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    inicio = time.time()\n",
    "    try:\n",
    "        respuesta_texto = funcion_generadora()\n",
    "        tiempo_respuesta = time.time() - inicio\n",
    "        \n",
    "        fragmento = respuesta_texto[:30].strip().replace(\"\\n\", \" \") + \"...\"\n",
    "\n",
    "        print(f\"‚úÖ  Respuesta generada en {tiempo_respuesta:.2f}s\")\n",
    "        print(f\"üìù  Fragmento: {fragmento}\")\n",
    "\n",
    "        evaluacion = evaluar_calidad_respuesta(prompt, respuesta_texto)\n",
    "\n",
    "        return {\n",
    "            'proveedor': nombre_proveedor,\n",
    "            'tiempo': tiempo_respuesta,\n",
    "            'Caracteres': len(respuesta_texto),\n",
    "            'puntuacion_calidad': evaluacion.puntuacion_calidad,\n",
    "            'razonamiento_calidad': evaluacion.razonamiento,\n",
    "            'exito': True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå  Error generando respuesta con {nombre_proveedor}: {e}\")\n",
    "        return {'proveedor': nombre_proveedor, 'exito': False}\n",
    "\n",
    "\n",
    "def comparar_modelos():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal que orquesta la comparaci√≥n de modelos.\n",
    "    \"\"\"\n",
    "    if not cliente_gemini:\n",
    "        print(\"\\nNo se puede continuar sin la configuraci√≥n del cliente de Gemini.\")\n",
    "        return\n",
    "\n",
    "    prompt = \"\"\"\n",
    "El Reto del Tri√°ngulo Viajero\n",
    "Un punto se encuentra inicialmente en el origen de un plano cartesiano, en la coordenada (0, 0). Este punto se mover√° para formar los v√©rtices de un tri√°ngulo ABC siguiendo estas reglas:\n",
    "\n",
    "Para llegar al V√©rtice A: El punto se desplaza desde el origen una distancia de 10 unidades en la misma direcci√≥n y sentido que el vector (3, 4).\n",
    "\n",
    "Para llegar al V√©rtice B: Desde el V√©rtice A, el punto se desplaza 7 unidades en la direcci√≥n negativa del eje Y (hacia abajo).\n",
    "\n",
    "El V√©rtice C se queda en el origen, (0, 0).\n",
    "\n",
    "Tu Misi√≥n:\n",
    "\n",
    "Responde a las siguientes preguntas, mostrando tus c√°lculos y razonamiento en cada paso.\n",
    "\n",
    "Calcula las coordenadas exactas de los V√©rtices A y B.\n",
    "\n",
    "Calcula el per√≠metro del tri√°ngulo ABC.\n",
    "\n",
    "Calcula el √°rea del tri√°ngulo ABC.\n",
    "\n",
    "Clasifica el tri√°ngulo (por ejemplo: is√≥sceles, escaleno, rect√°ngulo). Debes justificar tu respuesta bas√°ndote en las longitudes de sus lados o en sus √°ngulos.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    print(\"üèÅ COMPARACI√ìN DE RENDIMIENTO Y CALIDAD DE MODELOS üèÅ\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    cliente_ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "    proveedores = [\n",
    "        (\n",
    "            # Lambda actualizada para usar el patr√≥n client.models.generate_content\n",
    "            lambda: cliente_gemini.models.generate_content(\n",
    "                model='gemma-3-1b-it',\n",
    "                contents=[types.Content(parts=[types.Part.from_text(text=prompt)])],\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.0,\n",
    "                    max_output_tokens=400\n",
    "                )\n",
    "            ).text,\n",
    "            \"Gemma 3 1B (Nube)\"\n",
    "        ),\n",
    "        (\n",
    "            lambda: cliente_ollama.chat.completions.create(\n",
    "                model=\"hf.co/MaziyarPanahi/gemma-3-1b-it-GGUF:Q4_K_M\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=400\n",
    "            ).choices[0].message.content,\n",
    "            \"Gemma 3 1B (Local y Quant)\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    resultados = []\n",
    "    for funcion_gen, nombre in proveedores:\n",
    "        resultado = medir_y_evaluar(funcion_gen, nombre, prompt)\n",
    "        resultados.append(resultado)\n",
    "\n",
    "    # --- Mostrar Resumen Comparativo ---\n",
    "    print(\"\\nüèÜ RESUMEN COMPARATIVO FINAL\")\n",
    "    print(\"=\" * 85)\n",
    "    print(f\"{'Proveedor':<30} | {'Tiempo (s)':<12} | {'Caracteres':<8} | {'Calidad (1-100)':<16} | {'Estado'}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    exitosos = [r for r in resultados if r.get('exito')]\n",
    "    for r in resultados:\n",
    "        if r.get('exito'):\n",
    "            print(f\"{r['proveedor']:<30} | {r['tiempo']:<12.2f} | {r['Caracteres']:<8} | {r['puntuacion_calidad']:<16} | ‚úÖ\")\n",
    "        else:\n",
    "            print(f\"{r['proveedor']:<30} | {'--':<12} | {'--':<8} | {'--':<16} | ‚ùå Error\")\n",
    "\n",
    "    # --- An√°lisis de Resultados ---\n",
    "    if len(exitosos) > 1:\n",
    "        mas_rapido = min(exitosos, key=lambda x: x['tiempo'])\n",
    "        mejor_calidad = max(exitosos, key=lambda x: x['puntuacion_calidad'])\n",
    "\n",
    "        print(\"\\nüìà AN√ÅLISIS DE GANADORES:\")\n",
    "        print(f\"üöÄ M√°s R√°pido: {mas_rapido['proveedor']} ({mas_rapido['tiempo']:.2f}s)\")\n",
    "        print(f\"‚≠ê Mejor Calidad: {mejor_calidad['proveedor']} (Puntuaci√≥n: {mejor_calidad['puntuacion_calidad']}/10)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    comparar_modelos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 6. Problema: Asistente Financiero Local con Mejores Pr√°cticas\n",
    "\n",
    "\n",
    "  ### üéØ Objetivo\n",
    "\n",
    "  Crear un **Asistente Financiero** profesional usando **orientaci√≥n a objetos** que integre modelos locales y en la nube.\n",
    "\n",
    "\n",
    "  ### üìã Requisitos \n",
    "\n",
    "1. **M√∫ltiples proveedores**: Implementar la capacidad de utilizar diferentes proveedores de modelos de lenguaje, incluyendo Azure OpenAI, Google Gemini y un modelo local (Ollama).\n",
    "\n",
    "2. **Streaming en tiempo real**: Mostrar las respuestas del asistente en tiempo real, de manera que aparezcan progresivamente mientras se generan.\n",
    "\n",
    "3. **Gesti√≥n de historial**: Mantener el contexto de la conversaci√≥n almacenando hasta los √∫ltimos 10 mensajes intercambiados entre el usuario y el asistente.\n",
    "\n",
    "4. **Comandos especiales**: Implementar los siguientes comandos especiales para controlar el comportamiento del asistente:\n",
    "   - `/estadisticas`: Mostrar estad√≠sticas de la conversaci√≥n actual.\n",
    "   - `/limpiar`: Limpiar el historial de la conversaci√≥n.\n",
    "   - `/cambiar`: Cambiar el proveedor de modelos actualmente en uso.\n",
    "   - `/salir`: Terminar la conversaci√≥n y salir del programa.\n",
    "   - `/ayuda`: Mostrar la lista de comandos disponibles y su descripci√≥n.\n",
    "\n",
    "5. **Estad√≠sticas**: Realizar un seguimiento de las estad√≠sticas de la conversaci√≥n, incluyendo el conteo de mensajes enviados y la duraci√≥n de la conversaci√≥n.\n",
    "\n",
    "6. **Gesti√≥n de errores**: Manejar adecuadamente los errores y excepciones que puedan ocurrir durante la ejecuci√≥n del programa. Controlar las excepciones esperadas y proporcionar mensajes de error informativos al usuario. Lanzar excepciones personalizadas cuando sea necesario para un manejo de errores m√°s granular.\n",
    "\n",
    "7. **Encapsulamiento en clase ChatbotFinanciero**: Encapsular toda la funcionalidad del Asistente Financiero en una clase llamada `ChatbotFinanciero`. La clase debe contener los m√©todos necesarios para interactuar con los diferentes proveedores, gestionar el historial de la conversaci√≥n, manejar los comandos especiales y realizar el seguimiento de las estad√≠sticas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Estructura base del chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from google import genai\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Constantes para los proveedores\n",
    "\n",
    "class ChatbotFinanciero:\n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializar el chatbot con todas las configuraciones\"\"\"\n",
    "        # TODO: Definir prompt del sistema\n",
    "        # TODO: Inicializar historial de mensajes\n",
    "        # TODO: Inicializar proveedor_activo\n",
    "        # TODO: Inicializar estad√≠sticas\n",
    "        pass\n",
    "\n",
    "    def responder_azure(self, mensajes: list[dict]) -> str:\n",
    "        \"\"\"Responde usando Azure OpenAI con streaming\"\"\"\n",
    "        # TODO: Crear cliente AzureOpenAI\n",
    "        # TODO: Hacer llamada con streaming\n",
    "        # TODO: Imprimir respuesta en tiempo real\n",
    "        pass\n",
    "\n",
    "    def responder_gemini(self, mensajes: list[dict]) -> str:\n",
    "        \"\"\"Responde usando Google Gemini con streaming\"\"\"\n",
    "        # TODO: Crear cliente genai\n",
    "        # TODO: Convertir mensajes a formato Gemini\n",
    "        # TODO: Hacer llamada con streaming\n",
    "        # TODO: Imprimir respuesta en tiempo real\n",
    "        pass\n",
    "\n",
    "    def responder_ollama(self, mensajes: list[dict]) -> str:\n",
    "        \"\"\"Responde usando Ollama con streaming\"\"\"\n",
    "        # TODO: Crear cliente OpenAI apuntando a localhost:11434\n",
    "        # TODO: Hacer llamada con streaming\n",
    "        # TODO: Imprimir respuesta en tiempo real\n",
    "        pass\n",
    "\n",
    "    def configurar_proveedor(self, proveedor: str) -> None:\n",
    "        \"\"\"Configura el proveedor a usar\"\"\"\n",
    "        # TODO: Configurar proveedor_activo \n",
    "        pass\n",
    "\n",
    "    def responder(self, mensaje: str) -> str:\n",
    "        \"\"\"M√©todo principal - responde usando el proveedor configurado\"\"\"\n",
    "        # TODO: Verificar si hay un proveedor configurado, lanzar excepci√≥n ValueError si no lo hay\n",
    "        # TODO: A√±adir mensaje al historial\n",
    "        # TODO: Llamar a la funci√≥n correcta seg√∫n proveedor_activo\n",
    "        # TODO: Gestionar historial (limitar a 10 mensajes)\n",
    "        pass\n",
    "\n",
    "    def mostrar_estadisticas(self) -> None:\n",
    "        \"\"\"Muestra estad√≠sticas de la conversaci√≥n\"\"\"\n",
    "        # TODO: Calcular duraci√≥n, contar mensajes, mostrar m√©tricas\n",
    "        pass\n",
    "\n",
    "    def limpiar_historial(self) -> None:\n",
    "        \"\"\"Limpia el historial de mensajes\"\"\"\n",
    "        # TODO: Limpiar historial\n",
    "        pass\n",
    "\n",
    "    def cambiar_proveedor(self) -> None:\n",
    "        \"\"\"Cambia el proveedor activo\"\"\"\n",
    "        # TODO: Preguntar al usuario por el nuevo proveedor\n",
    "        # TODO: Llamar a configurar_proveedor() con el nuevo proveedor\n",
    "        pass\n",
    "\n",
    "    def mostrar_ayuda(self) -> None:\n",
    "        \"\"\"Muestra la ayuda con los comandos disponibles\"\"\"\n",
    "        # TODO: Imprimir lista de comandos y su descripci√≥n\n",
    "        pass\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Funci√≥n principal para el chatbot interactivo\"\"\"\n",
    "    # TODO: Crear instancia del chatbot\n",
    "    # TODO: Configurar proveedor inicial\n",
    "    # TODO: Mostrar comandos disponibles\n",
    "    # TODO: Bucle principal del chat\n",
    "    # TODO: Manejar comandos especiales (/estadisticas, /limpiar, /cambiar, /ayuda, /salir)\n",
    "    # TODO: Enviar mensaje y obtener respuesta\n",
    "    # TODO: Manejar excepciones y mostrar mensajes de error al usuario\n",
    "    pass\n",
    "\n",
    "# Crear una instancia del chatbot\n",
    "chatbot = ChatbotFinanciero()\n",
    "# Cambiar proveedor\n",
    "chatbot.cambiar_proveedor()\n",
    "# Responder a un mensaje\n",
    "respuesta = chatbot.responder(\"¬øCu√°l es la capital de Espa√±a?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## üéâ ¬°Felicitaciones!\n",
    "\n",
    "  Has completado el lab de **Modelos de IA Locales con Ollama y LM Studio**. Ahora tienes las habilidades para:\n",
    "\n",
    "  ‚úÖ Ejecutar modelos de IA localmente\n",
    "\n",
    "  ‚úÖ Gestionar modelos con herramientas profesionales\n",
    "\n",
    "  ‚úÖ Integrar modelos locales en aplicaciones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
