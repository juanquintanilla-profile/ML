{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dy8xvxFe7PwH"
   },
   "source": [
    "# Sesión práctica de introducción a los *generative large language models (LLMs)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TpKFCk97PwL"
   },
   "source": [
    "**Índice**\n",
    "- [Tipos de modelos de lenguaje y ecosistema actual](#toc0_)\n",
    "- [Primeros pasos con un modelo de lenguaje](#toc1_)\n",
    "- [Ejemplo práctico: no generativos vs instruction tuned VS NO instruction tuned** ](#toc12_)\n",
    "- [Estrategias y parámetros de generación](#toc2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" # Si no tenéis GPU cambiar a \"cpu\"\n",
    "# Si usáis un Google Colab ó un entorno con GPU podéis dejarlo en \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Útiles de programación que usaremos a lo largo del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers.generation.utils import GenerateDecoderOnlyOutput, GenerateBeamDecoderOnlyOutput\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def describir_modelo(model: AutoModelForCausalLM):\n",
    "    n_params = model.num_parameters()\n",
    "    print(f\" - El modelo tiene {n_params/1e6:,.1f} millones parámetros\") # ~0.5 billones (1 billon = 1,000 millones en inglés) de parámetros\n",
    "    print(f\" - Arquitectura: {model.config.architectures[0]}\")\n",
    "    try:\n",
    "        ctx = model.config.max_position_embeddings\n",
    "    except:\n",
    "        ctx = model.config.seq_length\n",
    "    print(f\" - Contexto máximo: {ctx:,} tokens\")\n",
    "    print(f\" - Formato de los parámetros: {model.config.torch_dtype}\")\n",
    "    print(f\" - Tamaño del vocabulario: {model.config.vocab_size:,} tokens\")\n",
    "\n",
    "\n",
    "def crear_prompt_chat_model(\n",
    "        mensaje_usuario: str,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        historial: str = None,\n",
    "        mensaje_sistema: str = \"You are a helpful assistant.\"\n",
    ") -> str:\n",
    "\n",
    "    if historial is None:\n",
    "        messages = [{\"role\": \"system\", \"content\": mensaje_sistema}]\n",
    "    else:\n",
    "        messages = []\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": mensaje_usuario})\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    if historial is not None:\n",
    "        prompt = f\"{historial} {prompt}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def responde_al_mensaje(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        prompt: str,\n",
    "        streamer: TextStreamer = None,\n",
    "        gen_kwargs: dict = None,\n",
    "        return_scores: bool = False,\n",
    "        device: str = \"cuda:0\"\n",
    "    ) -> list[str] | tuple[list[str], GenerateDecoderOnlyOutput]:\n",
    "\n",
    "    gen_kwargs = gen_kwargs or {}\n",
    "    if return_scores:\n",
    "        gen_kwargs[\"return_dict_in_generate\"] = True\n",
    "        gen_kwargs[\"output_scores\"] = True\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    model_output = model.generate(\n",
    "        input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **gen_kwargs,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    if isinstance(model_output, GenerateDecoderOnlyOutput):\n",
    "        # Será el caso si en model.generate hemos pasado return_dict_in_generate=True\n",
    "        response_tokens = model_output.sequences\n",
    "    elif isinstance(model_output, torch.Tensor):\n",
    "        response_tokens = model_output\n",
    "    elif isinstance(model_output, GenerateBeamDecoderOnlyOutput):\n",
    "        response_tokens = model_output.sequences\n",
    "    else:\n",
    "        raise ValueError(f\"El modelo ha devuelto un tipo inesperado: {type(model_output)}\")\n",
    "\n",
    "\n",
    "    responses_txt = tokenizer.batch_decode(response_tokens[:,len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "    return responses_txt, model_output\n",
    "\n",
    "\n",
    "def inspeccionar_probabilidades(\n",
    "        model_output: GenerateDecoderOnlyOutput,\n",
    "        response_txt: list[str],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        top_n_probables: int = 5\n",
    ") -> None:\n",
    "    response_tokens = tokenizer(response_txt, return_tensors=\"pt\").input_ids.to(DEVICE)[0]\n",
    "    # Añadimos el token de EoS (End of Sentence)\n",
    "    response_tokens = torch.cat([response_tokens, torch.tensor([tokenizer.eos_token_id], device=DEVICE)])\n",
    "\n",
    "    # Probabilidades de cada token en cada elemento de la respuesta\n",
    "    probs_por_paso = [nn.functional.softmax(scores, dim=-1) for scores in model_output.scores]\n",
    "\n",
    "    # Cada elemento de la lista `probs_por_paso` es un tensor de shape (1, n_vocab),\n",
    "    # y contiene la probabilidad de elegir cada uno de los tokens del vocabulario\n",
    "    print(f\"Probabilidades asociadas a cada token\", end='')\n",
    "    if top_n_probables != 0:\n",
    "        print(f\" - [Candidatos más probables]\", end='')\n",
    "    for token_elegido_paso_i, probs_paso_i in zip(response_tokens, probs_por_paso):\n",
    "        prob_of_tok_i = probs_paso_i[0,token_elegido_paso_i]\n",
    "        # Índices de los tokens más probables\n",
    "        print(f\"\\n  '{tokenizer.decode(token_elegido_paso_i)}'({prob_of_tok_i:.0%})\", end='')\n",
    "        if top_n_probables != 0:\n",
    "            print(\"- [\", end='')\n",
    "            top_n_tokens = torch.argsort(probs_paso_i[0], descending=True)[:top_n_probables]\n",
    "            probs_de_top_n = probs_paso_i[0,top_n_tokens]\n",
    "            # Excluir si alguna es 0\n",
    "            top_n_tokens = top_n_tokens[probs_de_top_n > 0]\n",
    "            probs_de_top_n = probs_de_top_n[probs_de_top_n > 0]\n",
    "            for token_idx in top_n_tokens:\n",
    "                token_txt = tokenizer.decode(token_idx)\n",
    "                print(f\"'{token_txt}'({probs_paso_i[0,token_idx]:.0%})\", end='')\n",
    "            print(\"]\", end='')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kcn7kHF7PwN"
   },
   "source": [
    "## <a id='toc0_'></a> Tipos de modelos de lenguaje y ecosistema actual\n",
    "\n",
    "Como vimos en el *fastbook*, los **modelos generativos** son un subconjunto (cada vez más importante, eso sí) de los foundational models de lenguaje, y no todos los modelos generativos saben seguir intrucciones / son conversacionales!\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://lh3.googleusercontent.com/d/1ZOMiZRqe1-JgRHd5VLrFrUIA9NSvSWhh\" width=\"600\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSddwANJ7PwR"
   },
   "source": [
    "Hoy nos centraremos sobre todo en los modelos de lenguaje generativo, con foco en los *instruction-tuned*, que son los más importantes y con más potencial a día de hoy.\n",
    "\n",
    "<!-- <div style=\"text-align:center\">\n",
    "    <img src=\"./imgs/LLMs_ecosystem.png\" width=\"600\">\n",
    "</div> -->\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://lh3.googleusercontent.com/d/1sDIJQNmbtU7QHG6TkfA728QubnEmTx0w\" width=\"600\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQBL1YVV7PwS"
   },
   "source": [
    "\n",
    "Como comentamos en el *fastbook*, en general, cuánto más grande es un modelo, más conocimiento y capacidad de \"razonar\", por eso ha habido tal explosión en el tamaño de estos:\n",
    "\n",
    "<!-- <div style=\"text-align:center\">\n",
    "    <img src=\"./imgs/model_size_vol.png\" width=\"700\">\n",
    "</div> -->\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://lh3.googleusercontent.com/d/1dNKHsHm9lsQsMNvqZmjUxVuqcN7YEDsj\" width=\"700\">\n",
    "</div>\n",
    "\n",
    "Para asegurar que todos los alumnos puedan correr los modelos y no tengan problemas con configurar el entorno, hardware, etc; vamos a usar un Google Colab como éste.\n",
    "El inconveniente es que el *tier* gratuito de Google sólo incluye una máquina con 12 GB de RAM y una GPU con 15 GB de VRAM, y que no permite instalar ciertas librerías que optimizarían la ejecucción de modelos más grandes. Por ello, en esta sesión práctica **usaremos modelos más pequeños, cuya calidad no es equiparable a los modelos que usaríamos en un sistema en producción, pero que nos servirán igualmente para ilustrar una serie de conceptos**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTqs3dxX7PwS"
   },
   "source": [
    "El modelo que usaremos hoy es de la familia [Qwen 2.5](https://qwenlm.github.io/blog/qwen2.5-llm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a> Primeros pasos con un modelo de lenguaje\n",
    "\n",
    "Recapitulamos: los modelos de lenguaje utilizan *tokenizers* ([link al visualizador](https://tiktokenizer.vercel.app/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traducción de la secuencia 'Hola modelito, hablas español?' a tokens:\n",
      "[68012, 1614, 6357, 11, 6055, 14493, 69888, 30]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "INSTRUCT_MODEL_CON_CASTELLANO = \"Qwen/Qwen2.5-0.5B-Instruct\" # ~2.4GB en la GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL_CON_CASTELLANO)\n",
    "\n",
    "# Veamos como tokeniza el textp\n",
    "\n",
    "prompt = f\"Hola modelito, hablas español?\" # En general, preguntar a un LLM sobre sí mismo nunca da respuestas fiables, pero nos sirve para este ejemplo\n",
    "prompt_tok = tokenizer.encode(prompt, return_tensors=\"pt\") # shape: (1, n_tokens)\n",
    "print(f\"Traducción de la secuencia '{prompt}' a tokens:\")\n",
    "toks_as_list = prompt_tok[0].numpy().tolist()\n",
    "print(toks_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68012 -> Hola\n",
      "1614 -> Ġmodel\n",
      "6357 -> ito\n",
      "11 -> ,\n",
      "6055 -> Ġhab\n",
      "14493 -> las\n",
      "69888 -> ĠespaÃ±ol\n",
      "30 -> ?\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab() # {txt: int}\n",
    "inv_vocab = {v: k for k, v in vocab.items()} # {int: txt}\n",
    "for tok_idx in toks_as_list:\n",
    "    print(f\"{tok_idx} -> {inv_vocab[tok_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: el tokenizador de los modelos Qwen (al menos hasta noviembre de 2024) se basa en el tokenizador basado en BPE (byte-pair-encoding) de OpenAI, que es *open-source*. Aunque con algunas modificaciones para que funcione mejor en Chino y otras lenguas. Mas info sobre el tokenizador en https://arxiv.org/pdf/2309.16609  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(INSTRUCT_MODEL_CON_CASTELLANO).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionemos algunas características del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - El modelo tiene 494.0 millones parámetros\n",
      " - Arquitectura: Qwen2ForCausalLM\n",
      " - Contexto máximo: 32,768 tokens\n",
      " - Formato de los parámetros: torch.bfloat16\n",
      " - Tamaño del vocabulario: 151,936 tokens\n"
     ]
    }
   ],
   "source": [
    "describir_modelo(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada modelo *instruction-tuned* tiene su propia plantilla de *prompting*, que hay que seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Hola modelito, hablas español?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "msj = f\"Hola modelito, hablas español?\" # En general, preguntar a un LLM sobre sí mismo nunca da respuestas fiables, pero nos sirve para este ejemplo\n",
    "prompt = crear_prompt_chat_model(msj, tokenizer)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generemos ahora nuestro primer mensaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí, estoy aquí para ayudarte. ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "prompt = crear_prompt_chat_model(\"Hola modelito, hablas español?\", tokenizer)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "gen_kwargs = {\"max_new_tokens\": 50}\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=streamer, gen_kwargs=gen_kwargs, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc12_'></a> Ejemplo práctico: no generativos vs NO instruction tuned vs instruction tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No generativos**\n",
    "\n",
    "Puedes probar [este modelo de HuggingFace](https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student) que es un clasificador de sentimiento entrenado en varios idiomas incluido el castellano. Por ejemplo, prueba con \"en esa fiesta había un ambiente muy turbio\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generativos NO instruction-tuned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_INSTRUCT_MODEL_CON_CASTELLANO = \"Qwen/Qwen2.5-0.5B\"\n",
    "no_instruct_model = AutoModelForCausalLM.from_pretrained(NO_INSTRUCT_MODEL_CON_CASTELLANO).to(DEVICE)\n",
    "no_instruct_tokenizer = AutoTokenizer.from_pretrained(NO_INSTRUCT_MODEL_CON_CASTELLANO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¿Cómo te llamas? ¿Qué haces aquí? ¿Qué haces aquí? ¿Qué haces aquí? ¿Qué haces aquí? ¿Qué haces aquí? ¿Qué haces aquí? ¿Qué haces aquí? ¿Qué\n"
     ]
    }
   ],
   "source": [
    "msj = \"Hola modelito, hablas español?\" # En general, preguntar a un LLM sobre sí mismo nunca da \n",
    "# respuestas fiables, pero nos sirve para este ejemplo\n",
    "response, model_output = responde_al_mensaje(\n",
    "    no_instruct_model, \n",
    "    no_instruct_tokenizer, \n",
    "    msj, \n",
    "    streamer=streamer, \n",
    "    gen_kwargs=gen_kwargs, \n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " un hombre que me miraba con una expresión de desagrado. Me pregunté si era un hombre de negocios o un hombre de calle. Me pregunté si era un hombre de negocios o un hombre de calle. Me\n"
     ]
    }
   ],
   "source": [
    "msj = \"Caminando por la calle me encontré a\"\n",
    "response, model_output = responde_al_mensaje(\n",
    "    no_instruct_model, \n",
    "    no_instruct_tokenizer, \n",
    "    msj, \n",
    "    streamer=streamer, \n",
    "    gen_kwargs=gen_kwargs, \n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generativos instruction-tuned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCT_MODEL_CON_CASTELLANO = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(INSTRUCT_MODEL_CON_CASTELLANO).to(DEVICE)\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL_CON_CASTELLANO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí, estoy aquí para ayudarte en español. ¿En qué puedo asistirte hoy?\n"
     ]
    }
   ],
   "source": [
    "msj = \"Hola modelito, hablas español?\" # En general, preguntar a un LLM sobre sí mismo nunca da \n",
    "# respuestas fiables, pero nos sirve para este ejemplo\n",
    "response, model_output = responde_al_mensaje(\n",
    "    instruct_model, \n",
    "    instruct_tokenizer, \n",
    "    crear_prompt_chat_model(msj, instruct_tokenizer),\n",
    "    streamer=streamer, \n",
    "    gen_kwargs=gen_kwargs, \n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caminando por la calle me encontré a una amiga en un parque.\n"
     ]
    }
   ],
   "source": [
    "msj = \"Completa esta frase: Caminando por la calle me encontré a\"\n",
    "response, model_output = responde_al_mensaje(\n",
    "    instruct_model, \n",
    "    instruct_tokenizer, \n",
    "    crear_prompt_chat_model(msj, instruct_tokenizer), \n",
    "    streamer=streamer, \n",
    "    gen_kwargs=gen_kwargs, \n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVO\n"
     ]
    }
   ],
   "source": [
    "msj = f\"\"\"\n",
    "En el mensaje a continuación, identifica si el sentido es POSITIVO, NEGATIVO o NEUTRAL. \n",
    "Responde sólo con una de estas tres palabras: POSITIVO; NEGATIVO; NEUTRAL; \n",
    "Mensaje: En esa fiesta había un ambiente muy turbio\n",
    "\"\"\"\n",
    "response, model_output = responde_al_mensaje(\n",
    "    instruct_model, \n",
    "    instruct_tokenizer, \n",
    "    crear_prompt_chat_model(msj, instruct_tokenizer), \n",
    "    streamer=streamer, \n",
    "    gen_kwargs=gen_kwargs, \n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGBPQHLv7PwU"
   },
   "source": [
    "## <a id='toc2_'></a>Estrategias y parámetros de generación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U88nlWvl7Pwa"
   },
   "source": [
    "Cómo funciona esta generación a más bajo nivel? La mayoría de modelos generativos utilizan por debajo\n",
    "un método llamado *multinomial sampling*\n",
    "\n",
    "<!-- <div style=\"text-align:center\">\n",
    "    <img src=\"./imgs/next_token_prob.png\" width=\"700\">\n",
    "</div> -->\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://lh3.googleusercontent.com/d/1M6q2UMXEYLsO1PbESozlMFb5xtm-_yfR\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link a visualizador](https://poloclub.github.io/transformer-explainer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXnilUGq7Pwa",
    "outputId": "f3ef9067-7e2e-4714-b140-2a8687736e60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    151645,\n",
       "    151643\n",
       "  ],\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_k\": 20,\n",
       "  \"top_p\": 0.8\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos si este en concreto hace multinomial sample por defecto\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYMZCjuh7Pwa"
   },
   "source": [
    "Vamos a ver como podemos inspeccionar las probabilidades de cada token en cada paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {'max_new_tokens': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1ObV5jX7Pwb",
    "outputId": "eaba82af-4462-4820-ed09-c26cdeee4aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí, estoy aquí para ayudarte. ¿Cómo puedo asistirte hoy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "prompt = crear_prompt_chat_model(\"Hola modelito, hablas español?\", tokenizer)\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=streamer, gen_kwargs=gen_kwargs, return_scores=True, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SsMdhXM7Pwb",
    "outputId": "9b48dab0-01f9-49ae-96bf-31d749eb5cd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades asociadas a cada token - [Candidatos más probables]\n",
      "  'S'(100%)- ['S'(100%)]\n",
      "  'í'(100%)- ['í'(100%)]\n",
      "  ','(100%)- [','(100%)]\n",
      "  ' estoy'(88%)- [' estoy'(88%)' hab'(12%)]\n",
      "  ' aquí'(72%)- [' aquí'(72%)' hab'(28%)]\n",
      "  ' para'(100%)- [' para'(100%)]\n",
      "  ' ayud'(100%)- [' ayud'(100%)]\n",
      "  'arte'(100%)- ['arte'(100%)]\n",
      "  '.'(38%)- [' en'(62%)'.'(38%)]\n",
      "  ' ¿'(100%)- [' ¿'(100%)]\n",
      "  'Cómo'(38%)- ['En'(62%)'Cómo'(38%)]\n",
      "  ' puedo'(100%)- [' puedo'(100%)]\n",
      "  ' as'(100%)- [' as'(100%)]\n",
      "  'ist'(100%)- ['ist'(100%)]\n",
      "  'ir'(100%)- ['ir'(100%)]\n",
      "  'te'(100%)- ['te'(100%)]\n",
      "  ' hoy'(100%)- [' hoy'(100%)]\n",
      "  '?'(100%)- ['?'(100%)]\n",
      "  '<|im_end|>'(100%)- ['<|im_end|>'(100%)]"
     ]
    }
   ],
   "source": [
    "inspeccionar_probabilidades(model_output, response, tokenizer, top_n_probables=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julio\n",
      "Probabilidades asociadas a cada token - [Candidatos más probables]\n",
      "  'Jul'(5%)- ['J'(48%)'Mar'(34%)'E'(7%)'ener'(6%)'Jul'(5%)]\n",
      "  'io'(0%)- ['uli'(50%)'ul'(50%)]\n",
      "  '<|im_end|>'(0%)- ['io'(100%)]"
     ]
    }
   ],
   "source": [
    "prompt = crear_prompt_chat_model(\"¿Cuál es el mejor mes del año? Responde con una sóla palabra!\", tokenizer)\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=streamer, gen_kwargs=gen_kwargs, return_scores=True, device=DEVICE)\n",
    "inspeccionar_probabilidades(model_output, response, tokenizer, top_n_probables=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caminando por la calle me encontré con un amigo en una cafetería.\n",
      "Probabilidades asociadas a cada token - [Candidatos más probables]\n",
      "  'c'(0%)- ['cam'(100%)]\n",
      "  'amin'(0%)- ['in'(100%)]\n",
      "  'ando'(100%)- ['ando'(100%)]\n",
      "  ' por'(100%)- [' por'(100%)]\n",
      "  ' la'(100%)- [' la'(100%)]\n",
      "  ' calle'(100%)- [' calle'(100%)]\n",
      "  ' me'(100%)- [' me'(100%)]\n",
      "  ' encontr'(100%)- [' encontr'(100%)]\n",
      "  'é'(100%)- ['é'(100%)]\n",
      "  ' con'(78%)- [' con'(78%)' en'(22%)]\n",
      "  ' un'(60%)- [' un'(60%)' una'(40%)]\n",
      "  ' amigo'(100%)- [' amigo'(100%)]\n",
      "  ' en'(39%)- [' en'(39%)'.'(30%)' que'(15%)' y'(8%)'.\n",
      "\n",
      "'(8%)]\n",
      "  ' una'(48%)- [' el'(52%)' una'(48%)]\n",
      "  ' caf'(32%)- [' caf'(32%)' ti'(31%)' de'(16%)' es'(8%)' par'(7%)' of'(6%)]\n",
      "  'eter'(100%)- ['eter'(100%)]\n",
      "  'ía'(100%)- ['ía'(100%)]\n",
      "  '.'(83%)- ['.'(83%)'.\n",
      "\n",
      "'(17%)]\n",
      "  '<|im_end|>'(100%)- ['<|im_end|>'(100%)]"
     ]
    }
   ],
   "source": [
    "prompt = crear_prompt_chat_model(\"Completa la frase: caminando por la calle me encontré\", tokenizer)\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=streamer, gen_kwargs=gen_kwargs, return_scores=True, device=DEVICE)\n",
    "inspeccionar_probabilidades(model_output, response, tokenizer, top_n_probables=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFBXAFCB7Pwc"
   },
   "source": [
    "Ahora hemos incluido la probabilidades en el output con fines didácticos. Pero hay algunos casos en que podemos usarlas como parte de nuestro algoritmo.\n",
    "\n",
    "Por cierto! el mensaje \"de sistema\", dependiendo del modelo, tiene más o menos impacto. Con el modelo que estamos usando, no tiene impacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTarzq2g7Pwc",
    "outputId": "0662bf6a-e2af-4462-de71-f4d5dd5c4eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí, estoy hablando en español. ¿En qué puedo ayudarte hoy?\n",
      "------------------------------------------------------------\n",
      "Sí, hago lo que me pides. ¿En qué puedo ayudarte hoy?\n",
      "------------------------------------------------------------\n",
      "Sí, estoy aquí para ayudarte. ¿Cómo puedo asistirte hoy?\n"
     ]
    }
   ],
   "source": [
    "msj = f\"Hola modelito, hablas español?\" # En general, preguntar a un LLM sobre sí mismo nunca da respuestas fiables, pero nos sirve para este ejemplo\n",
    "_ = responde_al_mensaje(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    crear_prompt_chat_model(msj, tokenizer, mensaje_sistema=\"jajksashajksdah\"),\n",
    "    streamer=streamer,\n",
    "    gen_kwargs=gen_kwargs,\n",
    "    device=DEVICE\n",
    "    \n",
    ")\n",
    "print(\"--\"*30)\n",
    "_ = responde_al_mensaje(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    crear_prompt_chat_model(msj, tokenizer, mensaje_sistema=\"Always answer with the word: 'banana'\"),\n",
    "    streamer=streamer,\n",
    "    gen_kwargs=gen_kwargs,\n",
    "    device=DEVICE\n",
    "\n",
    ")\n",
    "print(\"--\"*30)\n",
    "_ = responde_al_mensaje(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    crear_prompt_chat_model(msj, tokenizer, mensaje_sistema=\"You are an unhelpful assistant that gives stupid answers\"),\n",
    "    streamer=streamer,\n",
    "    gen_kwargs=gen_kwargs,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA_vd9ps7Pwd"
   },
   "source": [
    "Veamos ahora algunos parámetros útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRAlDWPk7Pwd",
    "outputId": "dcc064d9-dc35-411c-aade-be7cbc685d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí, estoy aquí para ayudarte en español. ¿Cómo puedo asistirte hoy?\n",
      "\n",
      "\n",
      "Probabilidades asociadas a cada token - [Candidatos más probables]\n",
      "  'S'(100%)- ['S'(100%)]\n",
      "  'í'(100%)- ['í'(100%)]\n",
      "  ','(100%)- [','(100%)]\n",
      "  ' estoy'(88%)- [' estoy'(88%)' hab'(12%)]\n",
      "  ' aquí'(72%)- [' aquí'(72%)' hab'(28%)]\n",
      "  ' para'(100%)- [' para'(100%)]\n",
      "  ' ayud'(100%)- [' ayud'(100%)]\n",
      "  'arte'(100%)- ['arte'(100%)]\n",
      "  ' en'(62%)- [' en'(62%)'.'(38%)]\n",
      "  ' español'(100%)- [' español'(100%)]\n",
      "  '.'(100%)- ['.'(100%)]\n",
      "  ' ¿'(100%)- [' ¿'(100%)]\n",
      "  'Cómo'(22%)- ['En'(78%)'Cómo'(22%)]\n",
      "  ' puedo'(100%)- [' puedo'(100%)]\n",
      "  ' as'(100%)- [' as'(100%)]\n",
      "  'ist'(100%)- ['ist'(100%)]\n",
      "  'ir'(100%)- ['ir'(100%)]\n",
      "  'te'(100%)- ['te'(100%)]\n",
      "  ' hoy'(100%)- [' hoy'(100%)]\n",
      "  '?'(100%)- ['?'(100%)]\n",
      "  '<|im_end|>'(100%)- ['<|im_end|>'(100%)]"
     ]
    }
   ],
   "source": [
    "# Temperatura - Probemos con diferentes valores\n",
    "gen_kwargs = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\": 50\n",
    "}\n",
    "prompt = crear_prompt_chat_model(\"Hola modelito, hablas español?\", tokenizer)\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=streamer, gen_kwargs=gen_kwargs, return_scores=True, device=DEVICE)\n",
    "print(\"\\n\")\n",
    "inspeccionar_probabilidades(model_output, response, tokenizer, top_n_probables=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caminando por la calle me encontré una casa. \n",
      "\n",
      "La frase original es una traducción indirecta de inglés en español:\n",
      "\n",
      "\"Camino por la calle y encontré una casa.\"\n",
      "\n",
      "La respuesta completa de palabras y frases se ve\n",
      "\n",
      "\n",
      "Probabilidades asociadas a cada token - [Candidatos más probables]\n",
      "  'c'(5%)- ['cam'(56%)'¡'(11%)'¿'(8%)'\"'(8%)'c'(5%)'en'(5%)'Cam'(3%)'con'(3%)]\n",
      "  'amin'(0%)- ['in'(90%)'ino'(10%)]\n",
      "  'ando'(100%)- ['ando'(100%)]\n",
      "  ' por'(100%)- [' por'(100%)]\n",
      "  ' la'(100%)- [' la'(100%)]\n",
      "  ' calle'(100%)- [' calle'(100%)]\n",
      "  ' me'(100%)- [' me'(100%)]\n",
      "  ' encontr'(100%)- [' encontr'(100%)]\n",
      "  'é'(100%)- ['é'(100%)]\n",
      "  ' una'(5%)- [' con'(42%)' en'(23%)' un'(7%)' al'(7%)' una'(5%)' __'(5%)' a'(5%)' ac'(3%)' sent'(3%)]\n",
      "  ' casa'(11%)- [' gran'(15%)' casa'(11%)' her'(9%)' señ'(7%)' persona'(6%)' ti'(6%)' nueva'(6%)' ciudad'(5%)' mujer'(5%)' mult'(5%)]\n",
      "  '.'(15%)- ['.'(15%)' en'(14%)' nueva'(14%)' de'(10%)'.\n",
      "\n",
      "'(8%)','(8%)' abandon'(7%)' con'(6%)' vac'(5%)' bl'(5%)]\n",
      "  ' \n",
      "\n",
      "'(8%)- ['<|im_end|>'(81%)' \n",
      "\n",
      "'(8%)' ¿'(7%)' La'(4%)]\n",
      "  'La'(16%)- ['Esta'(48%)'La'(16%)'Este'(10%)'Exp'(8%)'P'(4%)'Explanation'(4%)'¿'(4%)'Just'(3%)'E'(3%)]\n",
      "  ' frase'(43%)- [' frase'(43%)' respuesta'(22%)' or'(8%)' trad'(7%)' pregunta'(7%)' estruct'(4%)' palabra'(3%)' complet'(3%)' fr'(3%)]\n",
      "  ' original'(4%)- [' completa'(40%)' se'(16%)' podría'(8%)' es'(8%)' puede'(7%)' está'(5%)' original'(4%)' parece'(4%)' que'(4%)' \"'(4%)]\n",
      "  ' es'(32%)- [' es'(32%)' podría'(10%)' se'(9%)' dice'(9%)' sería'(8%)' está'(6%)' menc'(5%)' puede'(5%)' \"'(5%)' indica'(4%)]\n",
      "  ' una'(18%)- [' una'(18%)' un'(16%)':\n",
      "\n",
      "'(11%)':'(11%)' incorrect'(10%)' correct'(8%)' \"'(7%)' de'(6%)' bastante'(5%)' muy'(4%)]\n",
      "  ' trad'(17%)- [' trad'(17%)' or'(14%)' versión'(13%)' forma'(8%)' loc'(7%)' par'(7%)' descri'(6%)' cita'(6%)' expres'(5%)' frase'(5%)]\n",
      "  'ucción'(100%)- ['ucción'(100%)]\n",
      "  ' indirect'(3%)- [' de'(22%)' al'(22%)' direct'(16%)' del'(9%)' natural'(8%)' en'(6%)' literal'(5%)' cl'(4%)' casual'(4%)' indirect'(3%)]\n",
      "  'a'(100%)- ['a'(100%)]\n",
      "  ' de'(49%)- [' de'(49%)' del'(26%)' al'(6%)' y'(6%)','(5%)' para'(5%)' hacia'(4%)]\n",
      "  ' inglés'(23%)- [' \"'(37%)' inglés'(23%)' la'(12%)' un'(11%)' una'(5%)' español'(5%)':\n",
      "\n",
      "'(4%)' el'(3%)]\n",
      "  ' en'(11%)- [' al'(44%)' en'(11%)' a'(10%)' para'(10%)' \"'(8%)','(7%)'.'(5%)' o'(5%)]\n",
      "  ' español'(43%)- [' español'(43%)' el'(20%)' la'(8%)' cast'(6%)' que'(6%)' catal'(4%)' may'(4%)' inglés'(4%)' este'(3%)' un'(3%)]\n",
      "  ':\n",
      "\n",
      "'(12%)- [','(38%)'.'(29%)':\n",
      "\n",
      "'(12%)' que'(6%)' y'(6%)' para'(5%)':'(4%)]\n",
      "  '\"'(52%)- ['\"'(52%)'Cam'(10%)'\"I'(7%)'-'(7%)'\"A'(5%)'\"C'(5%)'1'(4%)'\"In'(4%)'\"You'(3%)'\"When'(3%)]\n",
      "  'Cam'(41%)- ['Cam'(41%)'cam'(23%)'En'(7%)'Me'(7%)' Cam'(6%)'me'(5%)' cam'(3%)'El'(3%)'m'(3%)'Des'(3%)]\n",
      "  'ino'(32%)- ['in'(55%)'ino'(32%)'ina'(6%)'ine'(4%)'inar'(3%)]\n",
      "  ' por'(52%)- [' por'(52%)' rum'(7%)' ar'(7%)' cam'(7%)' en'(6%)' al'(5%)' and'(4%)' rec'(4%)' durante'(4%)' hacia'(4%)]\n",
      "  ' la'(85%)- [' la'(85%)' el'(15%)]\n",
      "  ' calle'(73%)- [' calle'(73%)' cal'(16%)' ciudad'(6%)' car'(5%)]\n",
      "  ' y'(0%)- ['les'(38%)'z'(31%)'le'(30%)]\n",
      "  ' encontr'(15%)- [','(31%)' me'(25%)' encontr'(15%)' y'(10%)' encuent'(8%)' he'(7%)' lleg'(3%)]\n",
      "  'é'(0%)- [' encontr'(35%)' me'(19%)' encuent'(18%)' al'(6%)' lleg'(5%)' ve'(4%)' vi'(4%)' en'(4%)' lo'(4%)]\n",
      "  ' una'(0%)- ['é'(100%)]\n",
      "  ' casa'(0%)- [' una'(85%)' un'(15%)]\n",
      "  '.\"\n",
      "\n",
      "'(0%)- [' casa'(80%)' cas'(10%)' habit'(10%)]\n",
      "  'La'(0%)- ['.\"\n",
      "\n",
      "'(70%)'.\"'(20%)'\"\n",
      "\n",
      "'(10%)]\n",
      "  ' respuesta'(0%)- ['Esta'(40%)'La'(10%)'En'(9%)'El'(6%)'Est'(5%)'Es'(5%)'Este'(5%)'Aqu'(4%)'Para'(4%)'P'(4%)]\n",
      "  ' completa'(0%)- [' trad'(26%)' frase'(20%)' respuesta'(9%)' interpret'(8%)' explic'(6%)' versión'(6%)' estruct'(6%)' palabra'(5%)' forma'(5%)' or'(5%)]\n",
      "  ' de'(0%)- [' completa'(36%)' correct'(12%)' se'(11%)' es'(8%)' al'(6%)' puede'(6%)' proporcion'(6%)' en'(5%)' está'(5%)' trad'(3%)]\n",
      "  ' palabras'(0%)- [' sería'(34%)' es'(18%)' se'(10%)','(7%)' podría'(6%)' en'(5%)' puede'(5%)' sigue'(4%)' de'(4%)' del'(4%)]\n",
      "  ' y'(0%)- [' \"'(31%)' la'(23%)' manera'(12%)' acuerdo'(12%)' esta'(8%)' las'(6%)' esa'(5%)' palabras'(4%)]\n",
      "  ' fr'(0%)- [' sería'(14%)' que'(14%)' es'(13%)' y'(12%)' en'(12%)' para'(8%)' se'(6%)' son'(6%)' correct'(5%)' podría'(4%)]\n",
      "  'ases'(0%)- [' fr'(33%)' or'(27%)' estruct'(11%)' sus'(8%)' su'(5%)' orden'(4%)' gram'(3%)' par'(3%)' expres'(3%)' sign'(3%)]\n",
      "  ' se'(0%)- ['ases'(100%)]\n",
      "  ' ve'(0%)- [' sería'(20%)' es'(17%)' que'(11%)' se'(9%)' podría'(8%)' son'(7%)' complet'(6%)','(6%)' en'(6%)' está'(5%)]\n",
      "  '<|im_end|>'(0%)- [' puede'(36%)' podría'(8%)' parece'(8%)' conv'(7%)' presenta'(6%)' dice'(6%)' proporcion'(5%)' debe'(5%)' comp'(4%)' ve'(4%)]"
     ]
    }
   ],
   "source": [
    "prompt = crear_prompt_chat_model(\"Completa la frase: caminando por la calle me encontré\", tokenizer)\n",
    "gen_kwargs = {\n",
    "    \"temperature\": 1.5,\n",
    "    \"max_new_tokens\": 50\n",
    "}\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=streamer, gen_kwargs=gen_kwargs, return_scores=True, device=DEVICE)\n",
    "print(\"\\n\")\n",
    "inspeccionar_probabilidades(model_output, response, tokenizer, top_n_probables=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkcOQ7iH7Pwd"
   },
   "source": [
    "Nota: una temperatura alta en modelos más potentes no tiene por qué resultar en respuestas de baja calidad, pero sí en respuestas más creativas y menos predecibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ffsf3MC7Pwe",
    "outputId": "5978ec0a-f056-4143-e9a9-b96c5fbca9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caminando por la calle me encontré con un amigo en una tienda de libros.\n",
      "Probabilidades asociadas a cada token - [Candidatos más probables]\n",
      "  'c'(0%)- ['cam'(100%)]\n",
      "  'amin'(0%)- ['in'(100%)]\n",
      "  'ando'(100%)- ['ando'(100%)]\n",
      "  ' por'(100%)- [' por'(100%)]\n",
      "  ' la'(100%)- [' la'(100%)]\n",
      "  ' calle'(100%)- [' calle'(100%)]\n",
      "  ' me'(100%)- [' me'(100%)]\n",
      "  ' encontr'(100%)- [' encontr'(100%)]\n",
      "  'é'(100%)- ['é'(100%)]\n",
      "  ' con'(78%)- [' con'(78%)' en'(22%)]\n",
      "  ' un'(60%)- [' un'(60%)' una'(40%)]\n",
      "  ' amigo'(100%)- [' amigo'(100%)]\n",
      "  ' en'(57%)- [' en'(57%)'.'(43%)]\n",
      "  ' una'(48%)- [' el'(52%)' una'(48%)]\n",
      "  ' ti'(49%)- [' caf'(51%)' ti'(49%)]\n",
      "  'enda'(100%)- ['enda'(100%)]\n",
      "  ' de'(100%)- [' de'(100%)]\n",
      "  ' libros'(40%)- [' '(60%)' libros'(40%)]\n",
      "  '.'(100%)- ['.'(100%)]\n",
      "  '<|im_end|>'(100%)- ['<|im_end|>'(100%)]"
     ]
    }
   ],
   "source": [
    "# Top-k sampling\n",
    "gen_kwargs = {\n",
    "    \"top_k\": 2,\n",
    "    \"max_new_tokens\": 50,\n",
    "}\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=streamer, gen_kwargs=gen_kwargs, return_scores=True, device=DEVICE)\n",
    "inspeccionar_probabilidades(model_output, response, tokenizer, top_n_probables=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxikOHZG7Pwe",
    "outputId": "c6d19eac-8859-446e-cc95-60bb87680ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caminando por la calle me encontré en mi casa.\n",
      "-----\n",
      "caminando por la calle me encontré con un amigo en una cafetería.\n",
      "\n",
      "Esta es una traducción simple y gramaticalmente correcta de la oración original. La frase se compone de dos partes:\n",
      "\n",
      "1. \"Camin\n",
      "-----\n",
      "caminando por la calle me encontré con una amiga en un restaurante local, disfrutando de una cena cómica y culinaria que había preparado ella mismo.\n"
     ]
    }
   ],
   "source": [
    "# Num return sequences\n",
    "gen_kwargs = {\n",
    "    \"num_return_sequences\": 3,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\": 50,\n",
    "}\n",
    "prompt = crear_prompt_chat_model(\"Completa la frase: caminando por la calle me encontré\", tokenizer)\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=None, gen_kwargs=gen_kwargs, return_scores=True, device=DEVICE)\n",
    "print(\"\\n-----\\n\".join(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyOCp-0g7Pwe"
   },
   "source": [
    "Hay otros muchos parámetros que influyen en la generación y que pueden ser útiles en un momento dado (ej. top_p, repetition_penalty,  etc)\n",
    "para un listado completo, podéis ver la documentación del método `generate` de la librería `transformers`:\n",
    "\n",
    "Además, es importante aclarar que estos parámetros generalmente son comunes a todos los modelos, incluido\n",
    "los modelos *closed-source*. Por ejemplo, en la documentación de la API de los modelos de OpenAI, se pueden encontrar\n",
    "muchos de estos parámetros: [link]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmO0V7FK7Pwf"
   },
   "source": [
    "**Greedy decoding**\n",
    "\n",
    "Consiste en seleccionar siempre el token con mayor probabilidad, en lugar de muestrear de la distribución multinomial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tt8EANJ97Pwf",
    "outputId": "81d7039e-0c83-4941-f857-e5764ea22781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aquí tienes uno:\n",
      "\n",
      "¿Por qué los gatos no usan Facebook?\n",
      "\n",
      "Para buscar \"gato\" en la página de Facebook.\n",
      "\n",
      "Espero que te guste! ¿Hay algo más?\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 200,\n",
    "}\n",
    "prompt = crear_prompt_chat_model(\"Cuéntame un chiste\", tokenizer)\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, streamer=streamer, gen_kwargs=gen_kwargs, return_scores=False, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EAY-L5P7Pwf"
   },
   "source": [
    "Notése que el *greedy decoding* es equivalente a multinomial sampling con *top_k=1* ó con *temperature=0.001*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvAy-pNs7Pwf"
   },
   "source": [
    "**Beam decoding**\n",
    "\n",
    "El *beam decoding* es una estrategia de generación que se usa para mejorar la calidad de las respuestas generadas por el modelo. En lugar de generar una única secuencia, el modelo genera varias secuencias candidatas y elige la \"mejor\" de ellas. La \"mejor\" de ellas en este caso se define como la secuencia con la mayor probabilidad conjunta de tokens (es decir, la que tiene el producto de sus probabilidades de tokens más alto).\n",
    "\n",
    "<!-- <div style=\"text-align:center\">\n",
    "    <img src=\"./imgs/beam_decoding.png\" width=\"700\">\n",
    "</div> -->\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"https://lh3.googleusercontent.com/d/1cGyQAMEn1LWR1RWbhnl3yqC8tYfqW_Lp\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGJHJZdF7Pwf",
    "outputId": "30ee6501-7448-4140-ac1e-b0eb69ba8b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aquí tienes un chiste para ti:\n",
      "\n",
      "¿Por qué los pájaros no usan Facebook?\n",
      "\n",
      "Porque ya tienen Instagram.\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"num_beams\": 5,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "# No se puede usar streamer si num_beams != 1\n",
    "response, model_output = responde_al_mensaje(model, tokenizer, prompt, gen_kwargs=gen_kwargs, return_scores=True, device=DEVICE)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWOxT0dy7Pwo"
   },
   "source": [
    "Hay otros métodos de decoding más sofisticados. Por ejemplo, contrastive search (más detalles en este post de [huggingface](https://huggingface.co/blog/introducing-csearch))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "unir_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ed8e06c21b04f19a73846520cb635f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_168ad46c4f6a4313afbe76ca7fd3a7dd",
      "placeholder": "​",
      "style": "IPY_MODEL_e71568204b0c445ea71cd8d054898f68",
      "value": "barcenas-mistral-7b.Q6_K.gguf: 100%"
     }
    },
    "168ad46c4f6a4313afbe76ca7fd3a7dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2df62079697941a1b8c0cc2144fd5eb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4d43b46d29254373872a97418f7745dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ed8e06c21b04f19a73846520cb635f9",
       "IPY_MODEL_e9a5c191cfb7478ebc8dff328b4f6bac",
       "IPY_MODEL_d89b1b19c5a24d64a5e2ac53649a09fc"
      ],
      "layout": "IPY_MODEL_a04b0866b213454b8308460b851dd6e1"
     }
    },
    "4fde216595e84eaba7004f18eaea6c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a04b0866b213454b8308460b851dd6e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a61f9ffeb0d0486e82e70ba1293fa4a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2ee653d63ee41708d4080f0ee2d0251": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d89b1b19c5a24d64a5e2ac53649a09fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a61f9ffeb0d0486e82e70ba1293fa4a1",
      "placeholder": "​",
      "style": "IPY_MODEL_4fde216595e84eaba7004f18eaea6c8b",
      "value": " 5.94G/5.94G [01:39&lt;00:00, 39.8MB/s]"
     }
    },
    "e71568204b0c445ea71cd8d054898f68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9a5c191cfb7478ebc8dff328b4f6bac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2ee653d63ee41708d4080f0ee2d0251",
      "max": 5942064768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2df62079697941a1b8c0cc2144fd5eb5",
      "value": 5942064768
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
